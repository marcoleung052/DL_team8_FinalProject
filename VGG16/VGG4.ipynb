{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "19dd142d-af4f-4cc4-a737-cb5cf859e92e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:41:18.244272Z",
          "iopub.status.busy": "2025-05-08T07:41:18.242351Z",
          "iopub.status.idle": "2025-05-08T07:41:18.340635Z",
          "shell.execute_reply": "2025-05-08T07:41:18.338962Z",
          "shell.execute_reply.started": "2025-05-08T07:41:18.244202Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a9dbc86fc832487883043e2302dde848",
            "78788a3bf8704d0487d21bd292c16280",
            "625c98ae8e0c4485b5a8b8dfa79aae12"
          ]
        },
        "id": "19dd142d-af4f-4cc4-a737-cb5cf859e92e",
        "outputId": "d42ffac4-846a-40dc-e8c1-9c14478ce2a6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9dbc86fc832487883043e2302dde848"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# ‰∏äÂÇ≥Ê™îÊ°àÔºàÊúÉË∑≥Âá∫Ê™îÊ°àÈÅ∏ÊìáÂô®Ôºâ\n",
        "from ipywidgets import FileUpload\n",
        "\n",
        "upload = FileUpload()\n",
        "display(upload)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "172906a3-1dd5-4484-a903-2877d1e50ebc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:41:25.642301Z",
          "iopub.status.busy": "2025-05-08T07:41:25.641367Z",
          "iopub.status.idle": "2025-05-08T07:41:25.678931Z",
          "shell.execute_reply": "2025-05-08T07:41:25.676984Z",
          "shell.execute_reply.started": "2025-05-08T07:41:25.642244Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "172906a3-1dd5-4484-a903-2877d1e50ebc",
        "outputId": "267f8d5e-6bff-41cc-83fa-235312fceeb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'metadata': {'name': 'kaggle.json', 'type': 'application/json', 'size': 64, 'lastModified': 1745215961986}, 'content': b'{\"username\":\"suchiwen\",\"key\":\"01a925cee9e9e9d232008524b0434fb9\"}'}\n",
            "kaggle.json Â∑≤ÊàêÂäüÂÑ≤Â≠òËá≥ /root/.kaggle/kaggle.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ÂÅáË®≠‰Ω†Âè™‰∏äÂÇ≥‰∫Ü‰∏ÄÂÄãÊ™îÊ°à\n",
        "# Check if any file is uploaded and get the filename\n",
        "if upload.value:\n",
        "    # Get the first (and likely only) filename from the dictionary keys\n",
        "    filename = list(upload.value.keys())[0]\n",
        "    fileinfo = upload.value[filename] # Access the file info dictionary using the filename as key\n",
        "else:\n",
        "    print(\"No file uploaded.\")\n",
        "    # Handle the case where no file is uploaded, perhaps by exiting or prompting the user.\n",
        "    # For this example, we'll assume a file was uploaded as per the traceback context.\n",
        "    raise FileNotFoundError(\"No file was uploaded.\")\n",
        "\n",
        "\n",
        "# È°ØÁ§∫ÂÖßÂÆπÁµêÊßãÔºàÈô§ÈåØÁî®Ôºâ\n",
        "print(fileinfo)\n",
        "\n",
        "# ÂÑ≤Â≠ò kaggle.json\n",
        "# filename is already obtained above\n",
        "content = fileinfo['content']\n",
        "\n",
        "kaggle_dir = Path.home() / \".kaggle\"\n",
        "kaggle_dir.mkdir(exist_ok=True)\n",
        "\n",
        "kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
        "with open(kaggle_json_path, \"wb\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "# Ë®≠ÂÆöÊ¨äÈôêÔºàLinux/macOS Âª∫Ë≠∞Ôºâ\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "print(f\"{filename} Â∑≤ÊàêÂäüÂÑ≤Â≠òËá≥ {kaggle_json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e13e384c-f12d-4c02-b891-80542dd9a6db",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:41:27.613483Z",
          "iopub.status.busy": "2025-05-08T07:41:27.612703Z",
          "iopub.status.idle": "2025-05-08T07:41:28.600387Z",
          "shell.execute_reply": "2025-05-08T07:41:28.597973Z",
          "shell.execute_reply.started": "2025-05-08T07:41:27.613424Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e13e384c-f12d-4c02-b891-80542dd9a6db",
        "outputId": "d0d2f359-57e7-4edf-fe0b-83217e20f1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                     title                                                  size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------  ----------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "fedesoriano/cifar100                                    CIFAR-100 Python                                  168517809  2020-12-26 08:37:10.143000          12526        178  1.0              \n",
            "pankrzysiu/cifar10-python                               CIFAR-10 Python                                   340613496  2018-01-27 13:42:40.967000          15066        255  0.75             \n",
            "petitbonney/cifar10-image-recognition                   CIFAR-10                                         1007971063  2019-10-01 12:50:23.227000           2976         27  0.8235294        \n",
            "valentynsichkar/cifar10-preprocessed                    CIFAR10 Preprocessed                             1227571899  2019-07-13 13:43:11.030000           3417         65  1.0              \n",
            "swaroopkml/cifar10-pngs-in-folders                      CIFAR-10 PNGs in folders                          293479104  2019-02-10 11:16:19.507000          13674         96  0.6875           \n",
            "ayush1220/cifar10                                       CIFAR-10                                          146259525  2022-01-11 21:03:48.427000           1432         13  0.6875           \n",
            "fedesoriano/cifar10-python-in-csv                       CIFAR-10 Python in CSV                            218807675  2021-06-22 09:07:55.670000           7195         53  1.0              \n",
            "alincijov/cifar-100                                     CIFAR-100                                         168517809  2020-09-13 13:40:25.763000            382         16  0.625            \n",
            "quanbk/cifar10                                          Cifar-10                                          170062484  2017-11-22 01:12:26.240000           3663         26  0.25             \n",
            "emadtolba/cifar10-comp                                  Cifar-10 comp                                     706709151  2018-09-09 02:55:22.263000            384         12  0.64705884       \n",
            "birdy654/cifake-real-and-ai-generated-synthetic-images  CIFAKE: Real and AI-Generated Synthetic Images    109625224  2023-03-28 16:00:29.500000          20625        152  0.875            \n",
            "amishfaldu/cifar10dataset                               Cifar-10 Dataset                                 4989376312  2020-10-07 04:30:40.600000            356          6  1.0              \n",
            "ibraheemmoosa/cifar100-256x256                          CIFAR100 256x256                                 2817065558  2021-04-24 17:09:57.140000           1040         23  0.875            \n",
            "jessicali9530/stl10                                     STL-10 Image Recognition Dataset                 2017846807  2018-06-11 03:02:24.153000           6901        125  0.75             \n",
            "gazu468/cifar10-classification-image                    Cifar10 Classification Image                      146259525  2022-10-29 18:47:18.453000            822         15  0.5              \n",
            "oxcdcd/cifar10                                          cifar10                                           183819211  2018-08-12 09:31:35.253000           3462         35  0.1875           \n",
            "pavansanagapati/cifar100                                CIFAR-100                                         337036008  2018-06-21 12:23:51.707000            417          4  0.3125           \n",
            "ekaakurniawan/the-cifar10-dataset                       The CIFAR-10 Dataset                              170063312  2021-04-06 16:57:56.220000            523          4  0.9375           \n",
            "pypiahmad/cifar-100                                     CIFAR-100                                         168517947  2023-10-29 09:53:59.520000             84         11  0.6875           \n",
            "imsparsh/musicnet-dataset                               MusicNet Dataset                                23086004822  2021-02-18 14:12:19.753000          16207        528  1.0              \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list -s cifar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2e5fb396-b2e8-4b41-a76c-81dcd0626c01",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:41:57.870318Z",
          "iopub.status.busy": "2025-05-08T07:41:57.868472Z",
          "iopub.status.idle": "2025-05-08T07:51:56.665414Z",
          "shell.execute_reply": "2025-05-08T07:51:56.664055Z",
          "shell.execute_reply.started": "2025-05-08T07:41:57.870233Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e5fb396-b2e8-4b41-a76c-81dcd0626c01",
        "outputId": "06b9d2be-1aec-41de-8452-c1198603dcf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.0\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "Downloading dog-breed-identification.zip to /content\n",
            " 95% 659M/691M [00:05<00:01, 31.2MB/s]\n",
            "100% 691M/691M [00:05<00:00, 122MB/s] \n"
          ]
        }
      ],
      "source": [
        "!pip install -U kaggle\n",
        "!pip install --upgrade pandas\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Âª∫Á´ã Kaggle Ë≥áÊñôÂ§æ\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# ‰∏ãËºâ Dog Breed Identification Ë≥áÊñôÈõÜ\n",
        "!kaggle competitions download -c dog-breed-identification --force\n",
        "!unzip -oq dog-breed-identification.zip -d dog-breed-identification\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "labels = pd.read_csv('dog-breed-identification/labels.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ÂÆâË£ù transformers Â•ó‰ª∂\n",
        "!pip install transformers\n",
        "\n",
        "# ÂåØÂÖ• CosineAnnealingWarmup\n",
        "from transformers import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owi2sPud_cFm",
        "outputId": "e55fbfac-5095-4618-e4a3-7072e1a79e4f"
      },
      "id": "owi2sPud_cFm",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from collections import Counter\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 32\n",
        "num_epochs = 25\n",
        "lr = 1e-4\n",
        "num_workers = 2\n",
        "val_ratio = 0.2\n",
        "gamma = 2.0"
      ],
      "metadata": {
        "id": "T880XKCTBx0H"
      },
      "id": "T880XKCTBx0H",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from collections import Counter\n",
        "import math\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Ëá™Ë®ÇÁâπÂæµÊèêÂèñÊ®°ÂûãÔºàMulti-InputÔºâ\n",
        "class MultiInputVGG(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultiInputVGG, self).__init__()\n",
        "        base = models.vgg16(pretrained=True)\n",
        "        self.backbone = nn.Sequential(*list(base.features.children()))\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.feature_dim = 512  # vgg16 ÊúÄÂæå conv Â±§Ëº∏Âá∫ channels\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim * 3, 512),  # Âêà‰Ωµ 3 ÂÄãÂ±ÄÈÉ®ÁâπÂæµ\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ê®°Êì¨Ëº∏ÂÖ•ÔºöËÄ≥„ÄÅÈºª„ÄÅÂ∞æ‰∏âÂÄã crop Êàñ augment ÂçÄÂüü\n",
        "        # È†êË®≠ÂÖ®ÈÉ®ÂæûÂêåÂºµÂúñÁîüÊàê‰∏çÂêå augment\n",
        "        feat1 = self.pool(self.backbone(x))\n",
        "        feat2 = self.pool(self.backbone(x.flip(-1)))\n",
        "        feat3 = self.pool(self.backbone(x[:, :, ::-1, :]))\n",
        "\n",
        "        feat1 = feat1.view(x.size(0), -1)\n",
        "        feat2 = feat2.view(x.size(0), -1)\n",
        "        feat3 = feat3.view(x.size(0), -1)\n",
        "\n",
        "        concat = torch.cat([feat1, feat2, feat3], dim=1)\n",
        "        out = self.classifier(concat)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ZvtNk_NZB2bY"
      },
      "id": "ZvtNk_NZB2bY",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CosineAnnealingWarmupLR\n",
        "class CosineAnnealingWarmupLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr=0.0, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps\n",
        "        self.min_lr = min_lr\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch < self.warmup_steps:\n",
        "            return [base_lr * self.last_epoch / self.warmup_steps for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            progress = (self.last_epoch - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
        "            return [self.min_lr + (base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress)) for base_lr in self.base_lrs]\n"
      ],
      "metadata": {
        "id": "LStHpBzxCss0"
      },
      "id": "LStHpBzxCss0",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Âü∫Êú¨ÂèÉÊï∏ ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 32\n",
        "num_epochs = 25\n",
        "lr = 1e-4\n",
        "num_workers = 2\n",
        "val_ratio = 0.2\n",
        "gamma = 2.0 # Focal Loss gamma"
      ],
      "metadata": {
        "id": "Y5faq0sMCwn3"
      },
      "id": "Y5faq0sMCwn3",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Ë≥áÊñôÈ†êËôïÁêÜ ----------\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(0.2, 0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n"
      ],
      "metadata": {
        "id": "qkzy6muWC15X"
      },
      "id": "qkzy6muWC15X",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Ëá™Ë®ÇÁâπÂæµÊèêÂèñÊ®°ÂûãÔºàMulti-InputÔºâ----------\n",
        "class MultiInputVGG(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultiInputVGG, self).__init__()\n",
        "        # Âä†ËºâÈ†êË®ìÁ∑¥ÁöÑ VGG16 Ê®°Âûã\n",
        "        base = models.vgg16(pretrained=True)\n",
        "        # ÊèêÂèñÁâπÂæµÊèêÂèñÂ±§\n",
        "        self.backbone = nn.Sequential(*list(base.features.children()))\n",
        "        # ‰ΩøÁî®Ëá™ÈÅ©ÊáâÂπ≥ÂùáÊ±†ÂåñÁ¢∫‰øùËº∏Âá∫Â∞∫ÂØ∏‰∏ÄËá¥\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # VGG16 features ÁöÑÊúÄÂæå‰∏ÄÂ±§Ëº∏Âá∫ÈÄöÈÅìÊï∏\n",
        "        self.feature_dim = 512\n",
        "\n",
        "        # ÂàÜÈ°ûÂô®ÔºåÊé•Êî∂Âêà‰ΩµÂæåÁöÑÁâπÂæµ\n",
        "        self.classifier = nn.Sequential(\n",
        "            # Áî±ÊñºÊ®°Êì¨‰∫Ü 3 ÂÄãËº∏ÂÖ•ÔºàÊàñÂêå‰∏ÄËº∏ÂÖ•ÁöÑ 3 ÂÄãÂ¢ûÂº∑ÁâàÊú¨ÔºâÔºåÁâπÂæµÁ∂≠Â∫¶ÁÇ∫ feature_dim * 3\n",
        "            nn.Linear(self.feature_dim * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ê®°Êì¨ËôïÁêÜÂêå‰∏ÄÂºµÂúñÁöÑ‰∏çÂêåÂçÄÂüüÊàñ‰∏çÂêåÂ¢ûÂº∑\n",
        "        # ÈÄôË£°Á∞°ÂñÆÂú∞Â∞çÂêå‰∏ÄËº∏ÂÖ•ÈÄ≤Ë°å‰∏çÂêåÁöÑÊìç‰Ωú‰æÜÊ®°Êì¨Â§öËº∏ÂÖ•\n",
        "        feat1 = self.pool(self.backbone(x))\n",
        "        # Ê∞¥Âπ≥ÁøªËΩâ\n",
        "        feat2 = self.pool(self.backbone(x.flip(-1)))\n",
        "        # ÂûÇÁõ¥ÁøªËΩâ\n",
        "        feat3 = self.pool(self.backbone(x.flip(-2))) # ‰ΩøÁî® flip(-2) ÈÄ≤Ë°åÂûÇÁõ¥ÁøªËΩâ\n",
        "\n",
        "        # Â∞áÊ±†ÂåñÂæåÁöÑÁâπÂæµÂ±ïÂπ≥\n",
        "        feat1 = feat1.view(x.size(0), -1)\n",
        "        feat2 = feat2.view(x.size(0), -1)\n",
        "        feat3 = feat3.view(x.size(0), -1)\n",
        "\n",
        "        # Âêà‰ΩµÁâπÂæµ\n",
        "        concat = torch.cat([feat1, feat2, feat3], dim=1)\n",
        "        # ÈÄöÈÅéÂàÜÈ°ûÂô®\n",
        "        out = self.classifier(concat)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "7kLs9qX5EwUK"
      },
      "id": "7kLs9qX5EwUK",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Dataset ----------\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "all_breeds = sorted(labels['breed'].unique())\n",
        "breed_to_idx = {breed: i for i, breed in enumerate(all_breeds)}\n",
        "idx_to_breed = {i: breed for breed, i in breed_to_idx.items()} # For later use in submission\n",
        "\n",
        "# Create a mapping from image id to class index\n",
        "id_to_class_idx = labels.set_index('id')['breed'].map(breed_to_idx).to_dict()\n",
        "\n",
        "# Custom Dataset for Dog Breed Identification\n",
        "class DogBreedDataset(Dataset):\n",
        "    def __init__(self, img_dir, id_to_class_idx, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.id_to_class_idx = id_to_class_idx\n",
        "        # Get list of image files and filter out non-jpg files or files not in labels.csv\n",
        "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg') and f[:-4] in id_to_class_idx]\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_id = img_name[:-4] # Remove .jpg extension\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        label = self.id_to_class_idx[img_id]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "full_dataset = DogBreedDataset(img_dir=\"dog-breed-identification/train\",\n",
        "                               id_to_class_idx=id_to_class_idx,\n",
        "                               transform=train_transform)\n",
        "\n",
        "val_size = int(len(full_dataset) * val_ratio)\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "class DatasetWithTransform(Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Retrieve the item from the underlying subset\n",
        "        x, y = self.subset[index]\n",
        "        # Apply the transform if it exists\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "# Redo the Dataset creation and splitting to handle transforms correctly for train and validation\n",
        "full_dataset_no_transform = DogBreedDataset(img_dir=\"dog-breed-identification/train\",\n",
        "                                             id_to_class_idx=id_to_class_idx,\n",
        "                                             transform=None) # No transform initially\n",
        "\n",
        "val_size = int(len(full_dataset_no_transform) * val_ratio)\n",
        "train_size = len(full_dataset_no_transform) - val_size\n",
        "\n",
        "# Split the dataset (subsets will return PIL Images)\n",
        "train_subset, val_subset = random_split(full_dataset_no_transform, [train_size, val_size])\n",
        "\n",
        "# Wrap the subsets to apply the specific transforms\n",
        "train_dataset = DatasetWithTransform(train_subset, transform=train_transform)\n",
        "val_dataset = DatasetWithTransform(val_subset, transform=val_transform)\n",
        "\n",
        "# Assuming the class mapping from the full dataset is needed later\n",
        "# You can access the original dataset from the subset if needed, or just use the id_to_class_idx directly\n",
        "class_to_idx = breed_to_idx # Define class_to_idx here"
      ],
      "metadata": {
        "id": "PWZmoZoXC3DF"
      },
      "id": "PWZmoZoXC3DF",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Sampler ----------\n",
        "# Access the indices from the underlying subset stored in the wrapper\n",
        "train_indices = train_dataset.subset.indices\n",
        "# Assuming dataset.targets is accessible from the original dataset the subset was created from\n",
        "# It is better to get the targets from the original full dataset before splitting\n",
        "# Let's assume `full_dataset_no_transform.id_to_class_idx` contains the mapping from image ID to class index.\n",
        "# We need to get the labels corresponding to the train_indices from the original data source.\n",
        "# A more robust way is to get the labels directly from the train_subset.\n",
        "train_labels = [full_dataset_no_transform.id_to_class_idx[full_dataset_no_transform.image_files[i][:-4]] for i in train_indices]\n",
        "\n",
        "\n",
        "label_counts = Counter(train_labels)\n",
        "class_sample_counts = [label_counts[i] for i in range(len(label_counts))]\n",
        "sample_weights = [1.0 / class_sample_counts[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "ze8cp5lxFPtT"
      },
      "id": "ze8cp5lxFPtT",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Focal Loss ----------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logpt = -self.ce(input, target)\n",
        "        pt = torch.exp(logpt)\n",
        "        loss = -((1 - pt) ** self.gamma) * logpt\n",
        "        return loss.mean()\n",
        "\n",
        "criterion = FocalLoss(gamma=gamma)"
      ],
      "metadata": {
        "id": "8QFX-WrWGAF4"
      },
      "id": "8QFX-WrWGAF4",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Ê®°ÂûãËàáÂÑ™ÂåñÂô® ----------\n",
        "model = MultiInputVGG(num_classes=len(label_counts)).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
        "total_steps = steps_per_epoch * num_epochs\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "scheduler = CosineAnnealingWarmupLR(optimizer, warmup_steps, total_steps)\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFyBVxEO_ji7",
        "outputId": "54750b90-a898-4018-b3a9-e8dfb04f54ba"
      },
      "id": "QFyBVxEO_ji7",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528M/528M [00:07<00:00, 73.1MB/s]\n",
            "<ipython-input-37-417fb633f242>:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Ë®ìÁ∑¥ ----------\n",
        "best_acc = 0\n",
        "train_losses, val_losses, val_accs = [], [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "print(\"\\nüìä Ë®ìÁ∑¥ÂÆåÊàêÁµ±Ë®àÔºö\")\n",
        "print(f\"Average Train Loss: {sum(train_losses)/len(train_losses):.4f}\")\n",
        "print(f\"Average Val Loss:   {sum(val_losses)/len(val_losses):.4f}\")\n",
        "print(f\"Average Val Acc:    {sum(val_accs)/len(val_accs):.4f}\")\n",
        "print(f\"Best Val Acc:       {best_acc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6CPhLjsGeIY",
        "outputId": "1e5f17cd-06a0-4a44-963e-97bf6215695c"
      },
      "id": "Q6CPhLjsGeIY",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-29dbda14626e>:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train Loss: 4.7115 | Val Loss: 4.7018 | Val Acc: 0.0113\n",
            "[Epoch 2] Train Loss: 4.6899 | Val Loss: 4.6446 | Val Acc: 0.0259\n",
            "[Epoch 3] Train Loss: 4.5782 | Val Loss: 4.2643 | Val Acc: 0.0558\n",
            "[Epoch 4] Train Loss: 4.0426 | Val Loss: 3.0113 | Val Acc: 0.2216\n",
            "[Epoch 5] Train Loss: 3.4735 | Val Loss: 2.3312 | Val Acc: 0.3307\n",
            "[Epoch 6] Train Loss: 3.1162 | Val Loss: 1.9050 | Val Acc: 0.4031\n",
            "[Epoch 7] Train Loss: 2.8643 | Val Loss: 1.6060 | Val Acc: 0.4628\n",
            "[Epoch 8] Train Loss: 2.6764 | Val Loss: 1.3943 | Val Acc: 0.5024\n",
            "[Epoch 9] Train Loss: 2.4807 | Val Loss: 1.1685 | Val Acc: 0.5431\n",
            "[Epoch 10] Train Loss: 2.2532 | Val Loss: 1.0623 | Val Acc: 0.5592\n",
            "[Epoch 11] Train Loss: 2.0947 | Val Loss: 0.9800 | Val Acc: 0.5690\n",
            "[Epoch 12] Train Loss: 1.9559 | Val Loss: 0.8736 | Val Acc: 0.6008\n",
            "[Epoch 13] Train Loss: 1.8727 | Val Loss: 0.8461 | Val Acc: 0.5954\n",
            "[Epoch 14] Train Loss: 1.7899 | Val Loss: 0.7109 | Val Acc: 0.6282\n",
            "[Epoch 15] Train Loss: 1.6420 | Val Loss: 0.6826 | Val Acc: 0.6370\n",
            "[Epoch 16] Train Loss: 1.6254 | Val Loss: 0.6372 | Val Acc: 0.6429\n",
            "[Epoch 17] Train Loss: 1.4809 | Val Loss: 0.6437 | Val Acc: 0.6477\n",
            "[Epoch 18] Train Loss: 1.4681 | Val Loss: 0.5937 | Val Acc: 0.6531\n",
            "[Epoch 19] Train Loss: 1.4157 | Val Loss: 0.5846 | Val Acc: 0.6629\n",
            "[Epoch 20] Train Loss: 1.3943 | Val Loss: 0.5710 | Val Acc: 0.6654\n",
            "[Epoch 21] Train Loss: 1.3560 | Val Loss: 0.5618 | Val Acc: 0.6698\n",
            "[Epoch 22] Train Loss: 1.3246 | Val Loss: 0.5652 | Val Acc: 0.6673\n",
            "[Epoch 23] Train Loss: 1.3171 | Val Loss: 0.5534 | Val Acc: 0.6678\n",
            "[Epoch 24] Train Loss: 1.3102 | Val Loss: 0.5436 | Val Acc: 0.6722\n",
            "[Epoch 25] Train Loss: 1.2772 | Val Loss: 0.5427 | Val Acc: 0.6737\n",
            "\n",
            "üìä Ë®ìÁ∑¥ÂÆåÊàêÁµ±Ë®àÔºö\n",
            "Average Train Loss: 2.3285\n",
            "Average Val Loss:   1.4392\n",
            "Average Val Acc:    0.5108\n",
            "Best Val Acc:       0.6737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- È†êÊ∏¨ ----------\n",
        "model.eval()\n",
        "predictions = []\n",
        "image_names_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image_names, images in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1).cpu().numpy()  # Softmax Ê©üÁéá\n",
        "        predictions.extend(probs)\n",
        "        image_names_list.extend(image_names)\n",
        "\n",
        "# ---------- Áî¢Áîü submission.csv ----------\n",
        "# ‰ª• class_to_idx Âª∫Á´ã idx_to_class\n",
        "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "# ÁÖß class index ÊéíÂ∫èÔºåÁ¢∫‰øùÊ¨Ñ‰ΩçÈ†ÜÂ∫è‰∏ÄËá¥\n",
        "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
        "\n",
        "submission_df = pd.DataFrame(predictions, columns=class_names)\n",
        "submission_df.insert(0, \"id\", [name for name in image_names_list])\n",
        "submission_df[\"id\"] = submission_df[\"id\"].str.replace(\".jpg\", \"\", regex=False)\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n",
        "print(\"‚úÖ Â∑≤Áî¢Âá∫Á¨¶ÂêàÊ†ºÂºèÁöÑ submission.csvÔºÅ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIEK7DZwGhJV",
        "outputId": "a78b311c-f9d7-4045-87c5-35fa6c1e5d0c"
      },
      "id": "lIEK7DZwGhJV",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Â∑≤Áî¢Âá∫Á¨¶ÂêàÊ†ºÂºèÁöÑ submission.csvÔºÅ\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9dbc86fc832487883043e2302dde848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_78788a3bf8704d0487d21bd292c16280",
            "metadata": [
              {
                "name": "kaggle.json",
                "type": "application/json",
                "size": 64,
                "lastModified": 1745215961986
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_625c98ae8e0c4485b5a8b8dfa79aae12"
          }
        },
        "78788a3bf8704d0487d21bd292c16280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "625c98ae8e0c4485b5a8b8dfa79aae12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}