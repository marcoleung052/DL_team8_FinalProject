{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "19dd142d-af4f-4cc4-a737-cb5cf859e92e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:41:18.244272Z",
          "iopub.status.busy": "2025-05-08T07:41:18.242351Z",
          "iopub.status.idle": "2025-05-08T07:41:18.340635Z",
          "shell.execute_reply": "2025-05-08T07:41:18.338962Z",
          "shell.execute_reply.started": "2025-05-08T07:41:18.244202Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e4f48ec944534c608d2fe395b1f3d28e",
            "68bed0b4b33d483ab2e8c59c48c44d8a",
            "c7fbbe662d7e42c6a5714f62ee8a059c"
          ]
        },
        "id": "19dd142d-af4f-4cc4-a737-cb5cf859e92e",
        "outputId": "e046ad5e-ef11-4b4a-ccb4-5db65bb2735f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4f48ec944534c608d2fe395b1f3d28e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# 上傳檔案（會跳出檔案選擇器）\n",
        "from ipywidgets import FileUpload\n",
        "\n",
        "upload = FileUpload()\n",
        "display(upload)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "172906a3-1dd5-4484-a903-2877d1e50ebc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:41:25.642301Z",
          "iopub.status.busy": "2025-05-08T07:41:25.641367Z",
          "iopub.status.idle": "2025-05-08T07:41:25.678931Z",
          "shell.execute_reply": "2025-05-08T07:41:25.676984Z",
          "shell.execute_reply.started": "2025-05-08T07:41:25.642244Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "172906a3-1dd5-4484-a903-2877d1e50ebc",
        "outputId": "36772136-0e8c-4bc3-8ba2-eab9d6b9193f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'metadata': {'name': 'kaggle.json', 'type': 'application/json', 'size': 64, 'lastModified': 1745215961986}, 'content': b'{\"username\":\"suchiwen\",\"key\":\"01a925cee9e9e9d232008524b0434fb9\"}'}\n",
            "kaggle.json 已成功儲存至 /root/.kaggle/kaggle.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 假設你只上傳了一個檔案\n",
        "# Check if any file is uploaded and get the filename\n",
        "if upload.value:\n",
        "    # Get the first (and likely only) filename from the dictionary keys\n",
        "    filename = list(upload.value.keys())[0]\n",
        "    fileinfo = upload.value[filename] # Access the file info dictionary using the filename as key\n",
        "else:\n",
        "    print(\"No file uploaded.\")\n",
        "    # Handle the case where no file is uploaded, perhaps by exiting or prompting the user.\n",
        "    # For this example, we'll assume a file was uploaded as per the traceback context.\n",
        "    raise FileNotFoundError(\"No file was uploaded.\")\n",
        "\n",
        "\n",
        "# 顯示內容結構（除錯用）\n",
        "print(fileinfo)\n",
        "\n",
        "# 儲存 kaggle.json\n",
        "# filename is already obtained above\n",
        "content = fileinfo['content']\n",
        "\n",
        "kaggle_dir = Path.home() / \".kaggle\"\n",
        "kaggle_dir.mkdir(exist_ok=True)\n",
        "\n",
        "kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
        "with open(kaggle_json_path, \"wb\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "# 設定權限（Linux/macOS 建議）\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "print(f\"{filename} 已成功儲存至 {kaggle_json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "e13e384c-f12d-4c02-b891-80542dd9a6db",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:41:27.613483Z",
          "iopub.status.busy": "2025-05-08T07:41:27.612703Z",
          "iopub.status.idle": "2025-05-08T07:41:28.600387Z",
          "shell.execute_reply": "2025-05-08T07:41:28.597973Z",
          "shell.execute_reply.started": "2025-05-08T07:41:27.613424Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e13e384c-f12d-4c02-b891-80542dd9a6db",
        "outputId": "24148e05-2d34-439f-c29f-38b77b62845f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                     title                                                  size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------  ----------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "fedesoriano/cifar100                                    CIFAR-100 Python                                  168517809  2020-12-26 08:37:10.143000          12302        176  1.0              \n",
            "pankrzysiu/cifar10-python                               CIFAR-10 Python                                   340613496  2018-01-27 13:42:40.967000          14829        252  0.75             \n",
            "petitbonney/cifar10-image-recognition                   CIFAR-10                                         1007971063  2019-10-01 12:50:23.227000           2890         27  0.8235294        \n",
            "valentynsichkar/cifar10-preprocessed                    CIFAR10 Preprocessed                             1227571899  2019-07-13 13:43:11.030000           3369         65  1.0              \n",
            "swaroopkml/cifar10-pngs-in-folders                      CIFAR-10 PNGs in folders                          293479104  2019-02-10 11:16:19.507000          13510         96  0.6875           \n",
            "fedesoriano/cifar10-python-in-csv                       CIFAR-10 Python in CSV                            218807675  2021-06-22 09:07:55.670000           7058         53  1.0              \n",
            "ayush1220/cifar10                                       CIFAR-10                                          146259525  2022-01-11 21:03:48.427000           1297         12  0.6875           \n",
            "alincijov/cifar-100                                     CIFAR-100                                         168517809  2020-09-13 13:40:25.763000            370         16  0.625            \n",
            "quanbk/cifar10                                          Cifar-10                                          170062484  2017-11-22 01:12:26.240000           3633         26  0.25             \n",
            "emadtolba/cifar10-comp                                  Cifar-10 comp                                     706709151  2018-09-09 02:55:22.263000            384         12  0.64705884       \n",
            "birdy654/cifake-real-and-ai-generated-synthetic-images  CIFAKE: Real and AI-Generated Synthetic Images    109625224  2023-03-28 16:00:29.500000          19862        150  0.875            \n",
            "amishfaldu/cifar10dataset                               Cifar-10 Dataset                                 4989376312  2020-10-07 04:30:40.600000            350          6  1.0              \n",
            "ibraheemmoosa/cifar100-256x256                          CIFAR100 256x256                                 2817065558  2021-04-24 17:09:57.140000           1036         23  0.875            \n",
            "jessicali9530/stl10                                     STL-10 Image Recognition Dataset                 2017846807  2018-06-11 03:02:24.153000           6851        125  0.75             \n",
            "oxcdcd/cifar10                                          cifar10                                           183819211  2018-08-12 09:31:35.253000           3406         35  0.1875           \n",
            "gazu468/cifar10-classification-image                    Cifar10 Classification Image                      146259525  2022-10-29 18:47:18.453000            782         14  0.5              \n",
            "pavansanagapati/cifar100                                CIFAR-100                                         337036008  2018-06-21 12:23:51.707000            416          4  0.3125           \n",
            "ekaakurniawan/the-cifar10-dataset                       The CIFAR-10 Dataset                              170063312  2021-04-06 16:57:56.220000            513          4  0.9375           \n",
            "pypiahmad/cifar-100                                     CIFAR-100                                         168517947  2023-10-29 09:53:59.520000             80         11  0.6875           \n",
            "imsparsh/musicnet-dataset                               MusicNet Dataset                                23086004822  2021-02-18 14:12:19.753000          16109        527  1.0              \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list -s cifar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "2e5fb396-b2e8-4b41-a76c-81dcd0626c01",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:41:57.870318Z",
          "iopub.status.busy": "2025-05-08T07:41:57.868472Z",
          "iopub.status.idle": "2025-05-08T07:51:56.665414Z",
          "shell.execute_reply": "2025-05-08T07:51:56.664055Z",
          "shell.execute_reply.started": "2025-05-08T07:41:57.870233Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e5fb396-b2e8-4b41-a76c-81dcd0626c01",
        "outputId": "591e934f-a205-41a8-8369-019f6b183ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "Downloading dog-breed-identification.zip to /content\n",
            " 97% 667M/691M [00:11<00:00, 101MB/s] \n",
            "100% 691M/691M [00:11<00:00, 64.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -U kaggle\n",
        "!pip install --upgrade pandas\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# 建立 Kaggle 資料夾\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 下載 Dog Breed Identification 資料集\n",
        "!kaggle competitions download -c dog-breed-identification --force\n",
        "!unzip -oq dog-breed-identification.zip -d dog-breed-identification\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "labels = pd.read_csv('dog-breed-identification/labels.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# ---------- 參數 ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 32\n",
        "num_epochs = 25\n",
        "lr = 1e-4\n",
        "num_workers = 2\n",
        "val_ratio = 0.2\n",
        "gamma = 2.0  # Focal Loss gamma\n",
        "\n",
        "# ---------- 資料增強 ----------\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n"
      ],
      "metadata": {
        "id": "WgWfz5uF5BT1"
      },
      "id": "WgWfz5uF5BT1",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "730c9a46-2b61-48be-b907-137ded3ab21d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:51:56.667647Z",
          "iopub.status.busy": "2025-05-08T07:51:56.667358Z",
          "iopub.status.idle": "2025-05-08T07:55:04.132388Z",
          "shell.execute_reply": "2025-05-08T07:55:04.130510Z",
          "shell.execute_reply.started": "2025-05-08T07:51:56.667621Z"
        },
        "tags": [],
        "id": "730c9a46-2b61-48be-b907-137ded3ab21d"
      },
      "outputs": [],
      "source": [
        "# ---------- Dataset 與切分 ----------\n",
        "dataset = ImageFolder(\"dog_images/train\", transform=train_transform)\n",
        "val_dataset = ImageFolder(\"dog_images/train\", transform=val_transform)\n",
        "\n",
        "val_size = int(len(dataset) * val_ratio)\n",
        "train_size = len(dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "val_dataset.dataset.transform = val_transform  # 替換 val transform\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 計算訓練集標籤權重\n",
        "train_indices = train_dataset.indices\n",
        "train_labels = [dataset.targets[i] for i in train_indices]\n",
        "label_counts = Counter(train_labels)"
      ],
      "metadata": {
        "id": "NQ0vxxqbGevl"
      },
      "id": "NQ0vxxqbGevl",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "53fafafb-66d1-417b-9603-84097d9b4686",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T07:55:04.135148Z",
          "iopub.status.busy": "2025-05-08T07:55:04.134501Z",
          "iopub.status.idle": "2025-05-08T07:55:05.342784Z",
          "shell.execute_reply": "2025-05-08T07:55:05.341362Z",
          "shell.execute_reply.started": "2025-05-08T07:55:04.135092Z"
        },
        "tags": [],
        "id": "53fafafb-66d1-417b-9603-84097d9b4686"
      },
      "outputs": [],
      "source": [
        "# ---------- 建立 Weighted Sampler ----------\n",
        "train_indices = train_dataset.indices\n",
        "train_labels = [dataset.targets[i] for i in train_indices]\n",
        "label_counts = Counter(train_labels)\n",
        "class_sample_counts = [label_counts[i] for i in range(len(label_counts))]\n",
        "sample_weights = [1.0 / class_sample_counts[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Dataloader ----------\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
      ],
      "metadata": {
        "id": "aURWPzszenMT"
      },
      "id": "aURWPzszenMT",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "dbbeeba0-bf0d-4bd6-9a0d-970074bdae22",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-08T08:08:55.826080Z",
          "iopub.status.busy": "2025-05-08T08:08:55.825635Z",
          "iopub.status.idle": "2025-05-08T08:08:55.833764Z",
          "shell.execute_reply": "2025-05-08T08:08:55.832792Z",
          "shell.execute_reply.started": "2025-05-08T08:08:55.825939Z"
        },
        "tags": [],
        "id": "dbbeeba0-bf0d-4bd6-9a0d-970074bdae22"
      },
      "outputs": [],
      "source": [
        "# ---------- Focal Loss ----------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        logpt = -self.ce(input, target)\n",
        "        pt = torch.exp(logpt)\n",
        "        focal_loss = -((1 - pt) ** self.gamma) * logpt\n",
        "        return focal_loss.mean()\n",
        "\n",
        "criterion = FocalLoss(gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 建立模型 ----------\n",
        "model = models.vgg16(pretrained=True)\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "model.classifier[6] = nn.Linear(model.classifier[6].in_features, len(label_counts))\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlUNtHeI7486",
        "outputId": "e08f2c90-5323-4ab2-e007-8f7e83abfcbd"
      },
      "id": "YlUNtHeI7486",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-8277400c2417>:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 訓練 ----------\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # ---------- 驗證 ----------\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX4MrdbL7J89",
        "outputId": "f8bd1ed4-16a3-499b-8d36-251aaa0a2f3d"
      },
      "id": "yX4MrdbL7J89",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-83-aac58b7a8314>:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train Loss: 1.1280 | Val Loss: 0.3360 | Val Acc: 0.7211\n",
            "[Epoch 2] Train Loss: 0.1316 | Val Loss: 0.2580 | Val Acc: 0.7613\n",
            "[Epoch 3] Train Loss: 0.0407 | Val Loss: 0.2363 | Val Acc: 0.7725\n",
            "[Epoch 4] Train Loss: 0.0162 | Val Loss: 0.2146 | Val Acc: 0.7823\n",
            "[Epoch 5] Train Loss: 0.0071 | Val Loss: 0.2164 | Val Acc: 0.7789\n",
            "[Epoch 6] Train Loss: 0.0043 | Val Loss: 0.2100 | Val Acc: 0.7916\n",
            "[Epoch 7] Train Loss: 0.0017 | Val Loss: 0.2113 | Val Acc: 0.7882\n",
            "[Epoch 8] Train Loss: 0.0014 | Val Loss: 0.2056 | Val Acc: 0.7896\n",
            "[Epoch 9] Train Loss: 0.0009 | Val Loss: 0.2074 | Val Acc: 0.7862\n",
            "[Epoch 10] Train Loss: 0.0008 | Val Loss: 0.2050 | Val Acc: 0.7896\n",
            "[Epoch 11] Train Loss: 0.0005 | Val Loss: 0.2093 | Val Acc: 0.7872\n",
            "[Epoch 12] Train Loss: 0.0003 | Val Loss: 0.2074 | Val Acc: 0.7886\n",
            "[Epoch 13] Train Loss: 0.0005 | Val Loss: 0.2056 | Val Acc: 0.7867\n",
            "[Epoch 14] Train Loss: 0.0003 | Val Loss: 0.2060 | Val Acc: 0.7882\n",
            "[Epoch 15] Train Loss: 0.0003 | Val Loss: 0.2076 | Val Acc: 0.7896\n",
            "[Epoch 16] Train Loss: 0.0002 | Val Loss: 0.2119 | Val Acc: 0.7901\n",
            "[Epoch 17] Train Loss: 0.0002 | Val Loss: 0.2099 | Val Acc: 0.7901\n",
            "[Epoch 18] Train Loss: 0.0002 | Val Loss: 0.2124 | Val Acc: 0.7916\n",
            "[Epoch 19] Train Loss: 0.0002 | Val Loss: 0.2114 | Val Acc: 0.7911\n",
            "[Epoch 20] Train Loss: 0.0002 | Val Loss: 0.2101 | Val Acc: 0.7945\n",
            "[Epoch 21] Train Loss: 0.0002 | Val Loss: 0.2101 | Val Acc: 0.7896\n",
            "[Epoch 22] Train Loss: 0.0001 | Val Loss: 0.2099 | Val Acc: 0.7877\n",
            "[Epoch 23] Train Loss: 0.0001 | Val Loss: 0.2101 | Val Acc: 0.7891\n",
            "[Epoch 24] Train Loss: 0.0002 | Val Loss: 0.2099 | Val Acc: 0.7882\n",
            "[Epoch 25] Train Loss: 0.0001 | Val Loss: 0.2100 | Val Acc: 0.7882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看類別分布\n",
        "val_labels = [dataset.targets[i] for i in val_dataset.indices]\n",
        "print(\"Train label distribution:\", Counter(train_labels))\n",
        "print(\"Val label distribution:\", Counter(val_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3FwAkdL6I0k",
        "outputId": "22b8e7d5-1032-416c-f840-6a5e1e79cf84"
      },
      "id": "p3FwAkdL6I0k",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train label distribution: Counter({100: 99, 97: 99, 42: 97, 1: 95, 73: 92, 94: 92, 7: 92, 26: 89, 87: 89, 11: 89, 117: 86, 75: 85, 52: 84, 88: 83, 68: 82, 6: 80, 13: 80, 10: 80, 3: 80, 9: 80, 69: 80, 101: 79, 78: 79, 109: 79, 61: 78, 82: 77, 86: 77, 59: 77, 30: 77, 70: 75, 80: 75, 55: 75, 84: 75, 40: 73, 93: 73, 17: 72, 56: 72, 114: 72, 60: 72, 19: 72, 35: 72, 92: 71, 98: 71, 63: 71, 39: 71, 15: 71, 119: 69, 2: 69, 14: 68, 71: 68, 33: 68, 8: 68, 90: 67, 96: 67, 74: 66, 102: 66, 110: 66, 105: 66, 28: 65, 116: 65, 64: 65, 18: 65, 76: 64, 58: 64, 67: 64, 79: 64, 62: 64, 107: 64, 118: 64, 57: 63, 5: 63, 37: 63, 106: 63, 20: 63, 115: 63, 31: 63, 50: 62, 0: 62, 85: 62, 38: 61, 77: 61, 104: 61, 111: 61, 95: 61, 12: 61, 41: 61, 21: 61, 32: 61, 54: 60, 4: 60, 36: 60, 53: 60, 47: 60, 108: 59, 51: 59, 49: 58, 34: 58, 81: 58, 44: 58, 25: 57, 65: 57, 16: 57, 29: 57, 113: 57, 103: 57, 22: 56, 99: 55, 66: 55, 89: 55, 91: 55, 24: 55, 72: 54, 48: 54, 83: 54, 27: 54, 112: 53, 45: 53, 46: 52, 23: 50, 43: 50})\n",
            "Val label distribution: Counter({109: 28, 3: 27, 97: 27, 61: 27, 52: 27, 93: 26, 69: 26, 9: 25, 73: 25, 95: 25, 11: 25, 102: 24, 59: 24, 20: 23, 6: 22, 87: 22, 13: 22, 27: 22, 53: 22, 54: 22, 84: 21, 99: 21, 90: 21, 1: 21, 91: 21, 60: 20, 80: 20, 67: 20, 81: 20, 57: 19, 17: 19, 79: 19, 33: 19, 72: 19, 50: 19, 24: 18, 104: 18, 0: 18, 7: 18, 25: 18, 111: 18, 42: 18, 118: 18, 28: 18, 98: 17, 77: 17, 45: 17, 94: 17, 46: 17, 26: 17, 89: 17, 62: 17, 68: 17, 112: 17, 37: 17, 31: 17, 75: 17, 35: 17, 64: 17, 14: 17, 2: 17, 55: 16, 51: 16, 23: 16, 43: 16, 66: 16, 12: 16, 36: 16, 115: 16, 56: 16, 101: 16, 116: 16, 30: 16, 63: 15, 76: 15, 86: 15, 96: 15, 39: 15, 19: 15, 48: 15, 16: 15, 83: 15, 70: 15, 47: 15, 5: 15, 4: 14, 15: 14, 21: 14, 107: 14, 44: 14, 34: 14, 41: 14, 74: 14, 8: 14, 110: 14, 103: 14, 58: 14, 29: 14, 100: 13, 114: 13, 119: 13, 92: 13, 38: 13, 32: 13, 85: 13, 71: 13, 105: 13, 78: 12, 113: 12, 88: 11, 22: 11, 65: 10, 40: 10, 108: 10, 18: 10, 82: 10, 10: 9, 106: 9, 117: 9, 49: 9})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# ---------- Data Transformation for Test Set ----------\n",
        "# Using the same transformation as the validation set\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std), # Assuming mean and std are defined\n",
        "])\n",
        "\n",
        "# ---------- Create Test Dataset and DataLoader ----------\n",
        "# Assuming your test images are in 'dog-breed-identification/test'\n",
        "# We need a custom dataset to get the image filenames\n",
        "class TestImageFolder(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted([os.path.join(root, f) for f in os.listdir(root) if f.endswith('.jpg')])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        # Return the filename without the directory path and the image tensor\n",
        "        return os.path.basename(img_path), image\n",
        "\n",
        "test_dataset = TestImageFolder(\"dog-breed-identification/test\", transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers) # Assuming batch_size and num_workers are defined\n",
        "\n",
        "# ---------- 預測 ----------\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    # The test_loader now yields image names and image tensors\n",
        "    for image_names, images in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "        # image_names is a list of filenames (strings)\n",
        "        for name, pred in zip(image_names, preds):\n",
        "            predictions.append((name, pred))\n",
        "\n",
        "# ---------- 產生 submission.csv ----------\n",
        "# 類別 index 對應類別名稱（以 train dataset 的 class_to_idx 為準）\n",
        "# Make sure 'dataset' is accessible and has the class_to_idx attribute\n",
        "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}  # 反轉\n",
        "submission = pd.DataFrame(predictions, columns=[\"id\", \"label\"])\n",
        "submission[\"label\"] = submission[\"label\"].map(idx_to_class)\n",
        "submission[\"id\"] = submission[\"id\"].str.replace(\".jpg\", \"\", regex=False)\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"✅ 已產出 submission.csv！\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaSTg70ghF_W",
        "outputId": "19305d62-8d9d-43c5-9bbf-02e641e49b9c"
      },
      "id": "PaSTg70ghF_W",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 已產出 submission.csv！\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e4f48ec944534c608d2fe395b1f3d28e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_68bed0b4b33d483ab2e8c59c48c44d8a",
            "metadata": [
              {
                "name": "kaggle.json",
                "type": "application/json",
                "size": 64,
                "lastModified": 1745215961986
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_c7fbbe662d7e42c6a5714f62ee8a059c"
          }
        },
        "68bed0b4b33d483ab2e8c59c48c44d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7fbbe662d7e42c6a5714f62ee8a059c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}