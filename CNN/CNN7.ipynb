{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a6a20b8ac34e4af7b58cf32abdb69ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_76ee803bc50541d9879e1ceb88a792ce",
            "metadata": [
              {
                "name": "kaggle.json",
                "type": "application/json",
                "size": 64,
                "lastModified": 1745215961986
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_62c99361a4f74f4e96e1877b91d2ae63"
          }
        },
        "76ee803bc50541d9879e1ceb88a792ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62c99361a4f74f4e96e1877b91d2ae63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a6a20b8ac34e4af7b58cf32abdb69ed4",
            "76ee803bc50541d9879e1ceb88a792ce",
            "62c99361a4f74f4e96e1877b91d2ae63"
          ]
        },
        "id": "QW8ijgN-7HJS",
        "outputId": "5897f186-295b-4fdd-b031-f1c8bcb8e3c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6a20b8ac34e4af7b58cf32abdb69ed4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# 上傳檔案（會跳出檔案選擇器）\n",
        "from ipywidgets import FileUpload\n",
        "\n",
        "upload = FileUpload()\n",
        "display(upload)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 假設你只上傳了一個檔案\n",
        "# Check if any file is uploaded and get the filename\n",
        "if upload.value:\n",
        "    # Get the first (and likely only) filename from the dictionary keys\n",
        "    filename = list(upload.value.keys())[0]\n",
        "    fileinfo = upload.value[filename] # Access the file info dictionary using the filename as key\n",
        "else:\n",
        "    print(\"No file uploaded.\")\n",
        "    # Handle the case where no file is uploaded, perhaps by exiting or prompting the user.\n",
        "    # For this example, we'll assume a file was uploaded as per the traceback context.\n",
        "    raise FileNotFoundError(\"No file was uploaded.\")\n"
      ],
      "metadata": {
        "id": "gPdMFpvTACZ_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 顯示內容結構（除錯用）\n",
        "print(fileinfo)\n",
        "\n",
        "# 儲存 kaggle.json\n",
        "# filename is already obtained above\n",
        "content = fileinfo['content']\n",
        "\n",
        "kaggle_dir = Path.home() / \".kaggle\"\n",
        "kaggle_dir.mkdir(exist_ok=True)\n",
        "\n",
        "kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
        "with open(kaggle_json_path, \"wb\") as f:\n",
        "    f.write(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwYmjMzxAHUT",
        "outputId": "a0d476a4-b149-42fc-8f82-8c6ab4343be9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'metadata': {'name': 'kaggle.json', 'type': 'application/json', 'size': 64, 'lastModified': 1745215961986}, 'content': b'{\"username\":\"suchiwen\",\"key\":\"01a925cee9e9e9d232008524b0434fb9\"}'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 設定權限（Linux/macOS 建議）\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "print(f\"{filename} 已成功儲存至 {kaggle_json_path}\")\n",
        "\n",
        "!kaggle datasets list -s cifar\n",
        "\n",
        "!pip install -U kaggle\n",
        "!pip install --upgrade pandas\n",
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhGXT3XJAQPq",
        "outputId": "30c75ae7-083f-45d7-ff4e-4b729a9e57dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json 已成功儲存至 /root/.kaggle/kaggle.json\n",
            "ref                                                     title                                                  size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------  ----------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "fedesoriano/cifar100                                    CIFAR-100 Python                                  168517809  2020-12-26 08:37:10.143000          12506        178  1.0              \n",
            "pankrzysiu/cifar10-python                               CIFAR-10 Python                                   340613496  2018-01-27 13:42:40.967000          15053        255  0.75             \n",
            "petitbonney/cifar10-image-recognition                   CIFAR-10                                         1007971063  2019-10-01 12:50:23.227000           2965         27  0.8235294        \n",
            "valentynsichkar/cifar10-preprocessed                    CIFAR10 Preprocessed                             1227571899  2019-07-13 13:43:11.030000           3417         65  1.0              \n",
            "swaroopkml/cifar10-pngs-in-folders                      CIFAR-10 PNGs in folders                          293479104  2019-02-10 11:16:19.507000          13667         96  0.6875           \n",
            "ayush1220/cifar10                                       CIFAR-10                                          146259525  2022-01-11 21:03:48.427000           1428         13  0.6875           \n",
            "fedesoriano/cifar10-python-in-csv                       CIFAR-10 Python in CSV                            218807675  2021-06-22 09:07:55.670000           7189         53  1.0              \n",
            "alincijov/cifar-100                                     CIFAR-100                                         168517809  2020-09-13 13:40:25.763000            381         16  0.625            \n",
            "quanbk/cifar10                                          Cifar-10                                          170062484  2017-11-22 01:12:26.240000           3663         26  0.25             \n",
            "emadtolba/cifar10-comp                                  Cifar-10 comp                                     706709151  2018-09-09 02:55:22.263000            384         12  0.64705884       \n",
            "birdy654/cifake-real-and-ai-generated-synthetic-images  CIFAKE: Real and AI-Generated Synthetic Images    109625224  2023-03-28 16:00:29.500000          20606        152  0.875            \n",
            "amishfaldu/cifar10dataset                               Cifar-10 Dataset                                 4989376312  2020-10-07 04:30:40.600000            356          6  1.0              \n",
            "ibraheemmoosa/cifar100-256x256                          CIFAR100 256x256                                 2817065558  2021-04-24 17:09:57.140000           1040         23  0.875            \n",
            "jessicali9530/stl10                                     STL-10 Image Recognition Dataset                 2017846807  2018-06-11 03:02:24.153000           6898        125  0.75             \n",
            "gazu468/cifar10-classification-image                    Cifar10 Classification Image                      146259525  2022-10-29 18:47:18.453000            822         15  0.5              \n",
            "oxcdcd/cifar10                                          cifar10                                           183819211  2018-08-12 09:31:35.253000           3461         35  0.1875           \n",
            "pavansanagapati/cifar100                                CIFAR-100                                         337036008  2018-06-21 12:23:51.707000            417          4  0.3125           \n",
            "ekaakurniawan/the-cifar10-dataset                       The CIFAR-10 Dataset                              170063312  2021-04-06 16:57:56.220000            523          4  0.9375           \n",
            "pypiahmad/cifar-100                                     CIFAR-100                                         168517947  2023-10-29 09:53:59.520000             84         11  0.6875           \n",
            "imsparsh/musicnet-dataset                               MusicNet Dataset                                23086004822  2021-02-18 14:12:19.753000          16205        528  1.0              \n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立 Kaggle 資料夾\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 下載 Dog Breed Identification 資料集\n",
        "!kaggle competitions download -c dog-breed-identification --force\n",
        "!unzip -oq dog-breed-identification.zip -d dog-breed-identification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgI_cKhrBjsP",
        "outputId": "5e308100-70cb-4af7-be3c-dbb92c18413d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "Downloading dog-breed-identification.zip to /content\n",
            " 95% 654M/691M [00:00<00:00, 1.24GB/s]\n",
            "100% 691M/691M [00:00<00:00, 1.26GB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "_FzoxTWcFWHP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Config & Seed =====================\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "5HygYGwbSgMG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Image Transform =====================\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomResizedCrop(64, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "Hcs7U_VkacuH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Region Extraction =====================\n",
        "def extract_regions(image_path, size=(64, 64)):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h, w, _ = img.shape\n",
        "\n",
        "    ear_roi = img[int(h * 0.0):int(h * 0.3), int(w * 0.25):int(w * 0.75)]\n",
        "    nose_roi = img[int(h * 0.3):int(h * 0.6), int(w * 0.35):int(w * 0.65)]\n",
        "    tail_roi = img[int(h * 0.7):int(h * 1.0), int(w * 0.3):int(w * 0.7)]\n",
        "\n",
        "    ear = cv2.resize(ear_roi, size)\n",
        "    nose = cv2.resize(nose_roi, size)\n",
        "    tail = cv2.resize(tail_roi, size)\n",
        "    return ear, tail, nose"
      ],
      "metadata": {
        "id": "NDb4u3Soy6p3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Dataset =====================\n",
        "class DogDataset(Dataset):\n",
        "    def __init__(self, ears, tails, noses, labels, transform=None):\n",
        "        self.ears = ears\n",
        "        self.tails = tails\n",
        "        self.noses = noses\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.transform:\n",
        "            ear = self.transform(self.ears[idx].permute(1, 2, 0).numpy())\n",
        "            tail = self.transform(self.tails[idx].permute(1, 2, 0).numpy())\n",
        "            nose = self.transform(self.noses[idx].permute(1, 2, 0).numpy())\n",
        "        else:\n",
        "            ear = self.ears[idx]\n",
        "            tail = self.tails[idx]\n",
        "            nose = self.noses[idx]\n",
        "        return (ear, tail, nose), self.labels[idx]\n",
        "\n",
        "class TestDogDataset(Dataset):\n",
        "    def __init__(self, ears, tails, noses, transform=None):\n",
        "        self.ears = ears\n",
        "        self.tails = tails\n",
        "        self.noses = noses\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ears)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.transform:\n",
        "            ear = self.transform(self.ears[idx].permute(1, 2, 0).numpy())\n",
        "            tail = self.transform(self.tails[idx].permute(1, 2, 0).numpy())\n",
        "            nose = self.transform(self.noses[idx].permute(1, 2, 0).numpy())\n",
        "        else:\n",
        "            ear = self.ears[idx]\n",
        "            tail = self.tails[idx]\n",
        "            nose = self.noses[idx]\n",
        "        return (ear, tail, nose)"
      ],
      "metadata": {
        "id": "YqMmPVzsS48W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 多輸入 CNN 模型定義\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class MultiInputCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultiInputCNN, self).__init__()\n",
        "\n",
        "        # 使用 VGG16 當作 backbone\n",
        "        self.base_model_ears = models.vgg16_bn(pretrained=True).features\n",
        "        self.base_model_tail = models.vgg16_bn(pretrained=True).features\n",
        "        self.base_model_nose = models.vgg16_bn(pretrained=True).features\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # 全連接層整合不同輸入特徵\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_ears, x_tail, x_nose):\n",
        "        f1 = self.global_pool(self.base_model_ears(x_ears))\n",
        "        f2 = self.global_pool(self.base_model_tail(x_tail))\n",
        "        f3 = self.global_pool(self.base_model_nose(x_nose))\n",
        "\n",
        "        f1 = f1.view(f1.size(0), -1)\n",
        "        f2 = f2.view(f2.size(0), -1)\n",
        "        f3 = f3.view(f3.size(0), -1)\n",
        "\n",
        "        combined = torch.cat([f1, f2, f3], dim=1)\n",
        "        output = self.classifier(combined)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "iilippBnBn3Y"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Smoothing 損失\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        confidence = 1.0 - self.smoothing\n",
        "        logprobs = F.log_softmax(pred, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "Y7rQ5UDaeotE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 虛擬資料集類別 (需改為實際資料讀取邏輯)\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class MultiInputDogDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, num_classes=120):\n",
        "        self.num_samples = num_samples\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 模擬耳、尾、鼻三張影像輸入，實際情況應讀取並預處理對應部位的影像\n",
        "        image_ears = torch.randn(3, 224, 224)\n",
        "        image_tail = torch.randn(3, 224, 224)\n",
        "        image_nose = torch.randn(3, 224, 224)\n",
        "        label = torch.randint(0, self.num_classes, (1,)).item()\n",
        "        return image_ears, image_tail, image_nose, label"
      ],
      "metadata": {
        "id": "1GIKJZUzUZpJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== Dummy Data Preparation =====================\n",
        "def prepare_pytorch_data(image_dir, labels_path, image_size=(64, 64)):\n",
        "    print(\"\\u26a0\\ufe0f Dummy data generated, replace with real data processing logic.\")\n",
        "    num_samples = 1000\n",
        "    dummy_shape = (num_samples, 3, image_size[0], image_size[1])\n",
        "    X_ear = torch.rand(dummy_shape)\n",
        "    X_tail = torch.rand(dummy_shape)\n",
        "    X_nose = torch.rand(dummy_shape)\n",
        "    y = torch.randint(0, 120, (num_samples,))\n",
        "    return X_ear, X_tail, X_nose, y\n",
        "\n",
        "def prepare_data_with_local(test_image_dir, test_df, image_size):\n",
        "    print(\"\\u26a0\\ufe0f Dummy test data generated, replace with real test logic.\")\n",
        "    num_test_samples = len(test_df)\n",
        "    dummy_shape = (num_test_samples, 3, image_size[0], image_size[1])\n",
        "    return torch.rand(dummy_shape), torch.rand(dummy_shape), torch.rand(dummy_shape)\n"
      ],
      "metadata": {
        "id": "Sx2AKm7hTUbA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練與驗證主程式\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "W6S7xy83UgCw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AMP\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ],
      "metadata": {
        "id": "hvfwlWbmU5ga"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate(model, train_loader, val_loader, device, num_epochs=10):\n",
        "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for x_ears, x_tail, x_nose, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n",
        "            x_ears, x_tail, x_nose, labels = x_ears.to(device), x_tail.to(device), x_nose.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(x_ears, x_tail, x_nose)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        avg_loss = total_loss / total\n",
        "        acc = correct / total\n",
        "        writer.add_scalar('Train/Loss', avg_loss, epoch)\n",
        "        writer.add_scalar('Train/Accuracy', acc, epoch)\n",
        "\n",
        "        # 驗證階段\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x_ears, x_tail, x_nose, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "                x_ears, x_tail, x_nose, labels = x_ears.to(device), x_tail.to(device), x_nose.to(device), labels.to(device)\n",
        "                with autocast():\n",
        "                    outputs = model(x_ears, x_tail, x_nose)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * labels.size(0)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_avg_loss = val_loss / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "        writer.add_scalar('Val/Loss', val_avg_loss, epoch)\n",
        "        writer.add_scalar('Val/Accuracy', val_acc, epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Acc={acc:.4f} | Val Loss={val_avg_loss:.4f}, Acc={val_acc:.4f}\")\n",
        "\n",
        "    writer.close()\n"
      ],
      "metadata": {
        "id": "aH6vT1H8U9UI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_classes = 120\n",
        "\n",
        "    model = MultiInputCNN(num_classes=num_classes).to(device)\n",
        "\n",
        "    train_dataset = MultiInputDogDataset(num_samples=1000, num_classes=num_classes)\n",
        "    val_dataset = MultiInputDogDataset(num_samples=200, num_classes=num_classes)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "    train_and_validate(model, train_loader, val_loader, device, num_epochs=10)\n",
        "\n",
        "    # 儲存模型\n",
        "    torch.save(model.state_dict(), \"multi_input_cnn.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SBS1b_HVDDn",
        "outputId": "2b0e7eca-8c8b-45c4-ba09-97eae4fe0508"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 246MB/s]\n",
            "<ipython-input-33-ba18014b5291>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Epoch 1/10 - Train:   0%|          | 0/32 [00:00<?, ?it/s]<ipython-input-33-ba18014b5291>:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/10 - Train: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n",
            "Validation:   0%|          | 0/7 [00:00<?, ?it/s]<ipython-input-33-ba18014b5291>:45: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss=4.7885, Acc=0.0050 | Val Loss=4.7839, Acc=0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 - Train: 100%|██████████| 32/32 [00:27<00:00,  1.16it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss=4.7902, Acc=0.0080 | Val Loss=4.7877, Acc=0.0050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 - Train: 100%|██████████| 32/32 [00:28<00:00,  1.13it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss=4.7883, Acc=0.0100 | Val Loss=4.7850, Acc=0.0050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 - Train: 100%|██████████| 32/32 [00:29<00:00,  1.09it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss=4.7892, Acc=0.0080 | Val Loss=4.7906, Acc=0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 - Train: 100%|██████████| 32/32 [00:29<00:00,  1.10it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss=4.7852, Acc=0.0090 | Val Loss=4.7891, Acc=0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 - Train: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss=4.7884, Acc=0.0070 | Val Loss=4.7843, Acc=0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 - Train: 100%|██████████| 32/32 [00:29<00:00,  1.10it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss=4.7882, Acc=0.0100 | Val Loss=4.7862, Acc=0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 - Train: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss=4.7874, Acc=0.0050 | Val Loss=4.7893, Acc=0.0050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 - Train: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss=4.7886, Acc=0.0030 | Val Loss=4.7865, Acc=0.0150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 - Train: 100%|██████████| 32/32 [00:28<00:00,  1.11it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:02<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Train Loss=4.7890, Acc=0.0090 | Val Loss=4.7880, Acc=0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 多輸入 CNN 模型定義\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class MultiInputCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MultiInputCNN, self).__init__()\n",
        "\n",
        "        # 使用 VGG16 當作 backbone\n",
        "        self.base_model_ears = models.vgg16_bn(pretrained=True).features\n",
        "        self.base_model_tail = models.vgg16_bn(pretrained=True).features\n",
        "        self.base_model_nose = models.vgg16_bn(pretrained=True).features\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # 全連接層整合不同輸入特徵\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_ears, x_tail, x_nose):\n",
        "        f1 = self.global_pool(self.base_model_ears(x_ears))\n",
        "        f2 = self.global_pool(self.base_model_tail(x_tail))\n",
        "        f3 = self.global_pool(self.base_model_nose(x_nose))\n",
        "\n",
        "        f1 = f1.view(f1.size(0), -1)\n",
        "        f2 = f2.view(f2.size(0), -1)\n",
        "        f3 = f3.view(f3.size(0), -1)\n",
        "\n",
        "        combined = torch.cat([f1, f2, f3], dim=1)\n",
        "        output = self.classifier(combined)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "WrhOx2iWAsjt"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LabelSmoothingCrossEntropy here or in a previous cell\n",
        "class LabelSmoothingCrossEntropy(torch.nn.Module):\n",
        "    def __init__(self, eps=0.1):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        num_classes = output.size(1)\n",
        "        log_probs = self.log_softmax(output)\n",
        "        target = target.long()\n",
        "        target = torch.nn.functional.one_hot(target, num_classes).float()\n",
        "        smooth_target = (1 - self.eps) * target + self.eps / num_classes\n",
        "        return (-smooth_target * log_probs).sum(dim=1).mean()\n"
      ],
      "metadata": {
        "id": "lRKeBjJZAuwS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "LmX96C4FBhGC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 測試資料預測與提交 CSV\n",
        "import pandas as pd\n",
        "import numpy as np # Make sure numpy is imported\n",
        "\n",
        "# Make sure test_df is defined before being used\n",
        "test_df = pd.read_csv('dog-breed-identification/sample_submission.csv')\n",
        "\n",
        "# Make sure image_size is defined before being used\n",
        "# Based on prepare_pytorch_data, image_size seems to be (64, 64)\n",
        "image_size = (64, 64)\n",
        "\n",
        "# Define X_test_ear, X_test_nose, X_test_tail by calling the function\n",
        "# prepare_data_with_local only returns 3 tensors for ear, tail, and nose.\n",
        "# Removed X_test_full from unpacking as it's not returned by the function.\n",
        "X_test_ear, X_test_nose, X_test_tail = prepare_data_with_local(\n",
        "    'dog-breed-identification/test', test_df, image_size\n",
        ")\n",
        "\n",
        "# 建立 Test Dataset & DataLoader\n",
        "# Make sure TestDogDataset and DataLoader are imported from torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Make sure val_transform is defined before being used\n",
        "# Assuming val_transform is defined in a previous cell as transforms.Compose([ ... ])\n",
        "from torchvision import transforms\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_dataset = TestDogDataset(X_test_ear, X_test_tail, X_test_nose, transform=val_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 預測\n",
        "# Make sure model and device are defined from previous cells\n",
        "# Assuming model is an instance of MultiInputCNN and device is set to 'cuda' or 'cpu'\n",
        "from tqdm import tqdm # Make sure tqdm is imported\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x_ears, x_tail, x_nose in tqdm(test_loader, desc=\"Testing\"):\n",
        "        x_ears, x_tail, x_nose = x_ears.to(device), x_tail.to(device), x_nose.to(device)\n",
        "        outputs = model(x_ears, x_tail, x_nose)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        all_preds.append(probs.cpu().numpy())\n",
        "\n",
        "# 整合預測\n",
        "all_preds = np.vstack(all_preds)\n",
        "\n",
        "# 建立 submission\n",
        "# Make sure labels_df and breeds are defined\n",
        "labels_df = pd.read_csv('dog-breed-identification/labels.csv')\n",
        "breeds = sorted(labels_df['breed'].unique())\n",
        "\n",
        "submission = pd.DataFrame(all_preds, columns=breeds)\n",
        "submission.insert(0, 'id', test_df['id'])\n",
        "submission.to_csv('submission_multi_input_cnn.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4rI3Vm0aipv",
        "outputId": "dffff9e8-a7ed-41ef-ed2a-9010fd6c14fd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Dummy test data generated, replace with real test logic.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 324/324 [00:23<00:00, 14.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#測試資料預測與提交 CSV\n",
        "test_df = pd.read_csv('dog-breed-identification/sample_submission.csv')\n",
        "X_test_full, X_test_ear, X_test_nose, X_test_tail = prepare_data_with_local(\n",
        "    'dog-breed-identification/test', test_df, image_size\n",
        ")\n",
        "\n",
        "preds = model.predict([X_test_full, X_test_ear, X_test_nose, X_test_tail])\n",
        "submission = pd.DataFrame(preds, columns=pd.get_dummies(labels['breed']).columns)\n",
        "submission.insert(0, 'id', test_df['id'])\n",
        "submission.to_csv('submission_multi_input_cnn.csv', index=False)"
      ],
      "metadata": {
        "id": "2GghWYro-mXr"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}