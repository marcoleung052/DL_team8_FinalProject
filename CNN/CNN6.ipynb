{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d28466f4852245af909bbacea5f691d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_4278ec05fe684c06a83981ff8a316398",
            "metadata": [
              {
                "name": "kaggle.json",
                "type": "application/json",
                "size": 64,
                "lastModified": 1745215961986
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_c3b0bf63dfc644b8a6cb4cee017ff642"
          }
        },
        "4278ec05fe684c06a83981ff8a316398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3b0bf63dfc644b8a6cb4cee017ff642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d28466f4852245af909bbacea5f691d4",
            "4278ec05fe684c06a83981ff8a316398",
            "c3b0bf63dfc644b8a6cb4cee017ff642"
          ]
        },
        "id": "QW8ijgN-7HJS",
        "outputId": "a97edec7-ec28-474b-d88b-0fb298340814"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d28466f4852245af909bbacea5f691d4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# 上傳檔案（會跳出檔案選擇器）\n",
        "from ipywidgets import FileUpload\n",
        "\n",
        "upload = FileUpload()\n",
        "display(upload)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 假設你只上傳了一個檔案\n",
        "# Check if any file is uploaded and get the filename\n",
        "if upload.value:\n",
        "    # Get the first (and likely only) filename from the dictionary keys\n",
        "    filename = list(upload.value.keys())[0]\n",
        "    fileinfo = upload.value[filename] # Access the file info dictionary using the filename as key\n",
        "else:\n",
        "    print(\"No file uploaded.\")\n",
        "    # Handle the case where no file is uploaded, perhaps by exiting or prompting the user.\n",
        "    # For this example, we'll assume a file was uploaded as per the traceback context.\n",
        "    raise FileNotFoundError(\"No file was uploaded.\")\n"
      ],
      "metadata": {
        "id": "gPdMFpvTACZ_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 顯示內容結構（除錯用）\n",
        "print(fileinfo)\n",
        "\n",
        "# 儲存 kaggle.json\n",
        "# filename is already obtained above\n",
        "content = fileinfo['content']\n",
        "\n",
        "kaggle_dir = Path.home() / \".kaggle\"\n",
        "kaggle_dir.mkdir(exist_ok=True)\n",
        "\n",
        "kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
        "with open(kaggle_json_path, \"wb\") as f:\n",
        "    f.write(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwYmjMzxAHUT",
        "outputId": "70351ddf-8164-4774-e540-24d0bc1b41f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'metadata': {'name': 'kaggle.json', 'type': 'application/json', 'size': 64, 'lastModified': 1745215961986}, 'content': b'{\"username\":\"suchiwen\",\"key\":\"01a925cee9e9e9d232008524b0434fb9\"}'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 設定權限（Linux/macOS 建議）\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "print(f\"{filename} 已成功儲存至 {kaggle_json_path}\")\n",
        "\n",
        "!kaggle datasets list -s cifar\n",
        "\n",
        "!pip install -U kaggle\n",
        "!pip install --upgrade pandas\n",
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhGXT3XJAQPq",
        "outputId": "33aa4e74-ba05-4a65-f7c9-763f1692806c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json 已成功儲存至 /root/.kaggle/kaggle.json\n",
            "ref                                                     title                                                  size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------  ----------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "fedesoriano/cifar100                                    CIFAR-100 Python                                  168517809  2020-12-26 08:37:10.143000          12486        178  1.0              \n",
            "pankrzysiu/cifar10-python                               CIFAR-10 Python                                   340613496  2018-01-27 13:42:40.967000          15047        255  0.75             \n",
            "petitbonney/cifar10-image-recognition                   CIFAR-10                                         1007971063  2019-10-01 12:50:23.227000           2964         27  0.8235294        \n",
            "valentynsichkar/cifar10-preprocessed                    CIFAR10 Preprocessed                             1227571899  2019-07-13 13:43:11.030000           3415         65  1.0              \n",
            "swaroopkml/cifar10-pngs-in-folders                      CIFAR-10 PNGs in folders                          293479104  2019-02-10 11:16:19.507000          13651         96  0.6875           \n",
            "ayush1220/cifar10                                       CIFAR-10                                          146259525  2022-01-11 21:03:48.427000           1420         13  0.6875           \n",
            "fedesoriano/cifar10-python-in-csv                       CIFAR-10 Python in CSV                            218807675  2021-06-22 09:07:55.670000           7184         53  1.0              \n",
            "alincijov/cifar-100                                     CIFAR-100                                         168517809  2020-09-13 13:40:25.763000            381         16  0.625            \n",
            "quanbk/cifar10                                          Cifar-10                                          170062484  2017-11-22 01:12:26.240000           3659         26  0.25             \n",
            "emadtolba/cifar10-comp                                  Cifar-10 comp                                     706709151  2018-09-09 02:55:22.263000            384         12  0.64705884       \n",
            "birdy654/cifake-real-and-ai-generated-synthetic-images  CIFAKE: Real and AI-Generated Synthetic Images    109625224  2023-03-28 16:00:29.500000          20573        152  0.875            \n",
            "amishfaldu/cifar10dataset                               Cifar-10 Dataset                                 4989376312  2020-10-07 04:30:40.600000            356          6  1.0              \n",
            "ibraheemmoosa/cifar100-256x256                          CIFAR100 256x256                                 2817065558  2021-04-24 17:09:57.140000           1040         23  0.875            \n",
            "jessicali9530/stl10                                     STL-10 Image Recognition Dataset                 2017846807  2018-06-11 03:02:24.153000           6898        125  0.75             \n",
            "gazu468/cifar10-classification-image                    Cifar10 Classification Image                      146259525  2022-10-29 18:47:18.453000            822         15  0.5              \n",
            "oxcdcd/cifar10                                          cifar10                                           183819211  2018-08-12 09:31:35.253000           3459         35  0.1875           \n",
            "pavansanagapati/cifar100                                CIFAR-100                                         337036008  2018-06-21 12:23:51.707000            417          4  0.3125           \n",
            "imsparsh/musicnet-dataset                               MusicNet Dataset                                23086004822  2021-02-18 14:12:19.753000          16203        528  1.0              \n",
            "ekaakurniawan/the-cifar10-dataset                       The CIFAR-10 Dataset                              170063312  2021-04-06 16:57:56.220000            523          4  0.9375           \n",
            "pypiahmad/cifar-100                                     CIFAR-100                                         168517947  2023-10-29 09:53:59.520000             84         11  0.6875           \n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.25.8)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 建立 Kaggle 資料夾\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 下載 Dog Breed Identification 資料集\n",
        "!kaggle competitions download -c dog-breed-identification --force\n",
        "!unzip -oq dog-breed-identification.zip -d dog-breed-identification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgI_cKhrBjsP",
        "outputId": "dac3b923-d21d-4d84-e302-cefe081d3b41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "Downloading dog-breed-identification.zip to /content\n",
            " 88% 608M/691M [00:00<00:00, 1.19GB/s]\n",
            "100% 691M/691M [00:00<00:00, 778MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "_FzoxTWcFWHP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 定義模型 ---\n",
        "class MultiInputCNN(nn.Module):\n",
        "    def __init__(self, num_classes=120):\n",
        "        super(MultiInputCNN, self).__init__()\n",
        "\n",
        "        def feature_extractor():\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.BatchNorm2d(64),\n",
        "\n",
        "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.BatchNorm2d(128),\n",
        "\n",
        "                nn.Flatten()\n",
        "            )\n",
        "\n",
        "        self.ear_branch = feature_extractor()\n",
        "        self.tail_branch = feature_extractor()\n",
        "        self.nose_branch = feature_extractor()\n",
        "\n",
        "        # 計算flatten後的大小，假設輸入尺寸64x64\n",
        "        # 經兩次 MaxPool2d(2) -> size: 64x64 -> 16x16\n",
        "        # 最後channel為128\n",
        "        flattened_size = 128 * 16 * 16 * 3  # 三個 branch 串接\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(flattened_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, ear, tail, nose):\n",
        "        x1 = self.ear_branch(ear)\n",
        "        x2 = self.tail_branch(tail)\n",
        "        x3 = self.nose_branch(nose)\n",
        "        x = torch.cat((x1, x2, x3), dim=1)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "iilippBnBn3Y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 自訂 Dataset ---\n",
        "class DogDataset(Dataset):\n",
        "    def __init__(self, ears, tails, noses, labels):\n",
        "        self.ears = ears\n",
        "        self.tails = tails\n",
        "        self.noses = noses\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.ears[idx], self.tails[idx], self.noses[idx]), self.labels[idx]\n"
      ],
      "metadata": {
        "id": "JNZrLZPYPSK1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 測試資料 Dataset ---\n",
        "class TestDogDataset(Dataset):\n",
        "    def __init__(self, ears, tails, noses):\n",
        "        self.ears = ears\n",
        "        self.tails = tails\n",
        "        self.noses = noses\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ears)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.ears[idx], self.tails[idx], self.noses[idx]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zRSyoc9PZVr",
        "outputId": "274053c0-0f9d-4237-9086-e7484c83a68a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data for PyTorch... This is a placeholder function.\n",
            "You need to implement the logic to load images, extract ear, tail, nose regions, preprocess, and save as .npy files.\n",
            "Looking for images in: dog-breed-identification/train\n",
            "Looking for labels in: dog-breed-identification/labels.csv\n",
            "Dummy data created. Replace with real data processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiInputCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6e3A_dQPt6F",
        "outputId": "ffd3fa55-2305-4538-c068-a628a26dc452"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 資料準備函數（需自行替換為實際圖像處理） ---\n",
        "def prepare_pytorch_data(image_dir, labels_path, image_size=(64, 64)):\n",
        "    print(\"⚠️ 你必須自行實作此函數以萃取耳朵、尾巴、鼻子圖像區塊。\")\n",
        "    print(f\"暫時使用虛擬資料作為示範，目標圖片尺寸：{image_size}\")\n",
        "\n",
        "    num_samples = 1000  # 假設有1000筆訓練資料\n",
        "    dummy_shape = (num_samples, image_size[0], image_size[1], 3)\n",
        "\n",
        "    X_ear_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_tail_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_nose_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    y_dummy = np.random.randint(0, 120, num_samples).astype(np.int64)\n",
        "\n",
        "    np.save('X_ear.npy', X_ear_dummy)\n",
        "    np.save('X_tail.npy', X_tail_dummy)\n",
        "    np.save('X_nose.npy', X_nose_dummy)\n",
        "    np.save('labels.npy', y_dummy)\n",
        "def prepare_data_with_local(test_image_dir, test_df, image_size):\n",
        "    print(\"⚠️ 你必須自行實作此函數以萃取測試集耳朵、尾巴、鼻子圖像區塊。\")\n",
        "    num_test_samples = len(test_df)\n",
        "    dummy_shape = (num_test_samples, image_size[0], image_size[1], 3)\n",
        "\n",
        "    X_test_ear_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_test_nose_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_test_tail_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "\n",
        "    return X_test_ear_dummy, X_test_nose_dummy, X_test_tail_dummy"
      ],
      "metadata": {
        "id": "X9FxbEUqXrrV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 資料準備函數（需自行替換為實際圖像處理） ---\n",
        "def prepare_pytorch_data(image_dir, labels_path, image_size=(64, 64)):\n",
        "    print(\"⚠️ 你必須自行實作此函數以萃取耳朵、尾巴、鼻子圖像區塊。\")\n",
        "    print(f\"暫時使用虛擬資料作為示範，目標圖片尺寸：{image_size}\")\n",
        "\n",
        "    num_samples = 1000  # 假設有1000筆訓練資料\n",
        "    dummy_shape = (num_samples, image_size[0], image_size[1], 3)\n",
        "\n",
        "    X_ear_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_tail_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_nose_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    y_dummy = np.random.randint(0, 120, num_samples).astype(np.int64)\n",
        "\n",
        "    np.save('X_ear.npy', X_ear_dummy)\n",
        "    np.save('X_tail.npy', X_tail_dummy)\n",
        "    np.save('X_nose.npy', X_nose_dummy)\n",
        "    np.save('labels.npy', y_dummy)\n",
        "\n",
        "def prepare_data_with_local(test_image_dir, test_df, image_size):\n",
        "    print(\"⚠️ 你必須自行實作此函數以萃取測試集耳朵、尾巴、鼻子圖像區塊。\")\n",
        "    num_test_samples = len(test_df)\n",
        "    dummy_shape = (num_test_samples, image_size[0], image_size[1], 3)\n",
        "\n",
        "    X_test_ear_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_test_nose_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_test_tail_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "\n",
        "    return X_test_ear_dummy, X_test_nose_dummy, X_test_tail_dummy\n",
        "\n",
        "# --- 主要執行區域 ---\n",
        "if __name__ == \"__main__\":\n",
        "    pytorch_image_size = (64, 64)\n",
        "    download_dir = 'dog-breed-identification'\n",
        "    train_image_dir = os.path.join(download_dir, 'train')\n",
        "    labels_file = os.path.join(download_dir, 'labels.csv')\n",
        "\n",
        "    # 準備訓練資料\n",
        "    prepare_pytorch_data(train_image_dir, labels_file, image_size=pytorch_image_size)\n",
        "\n",
        "    # 載入資料並轉換 tensor (N,H,W,C) -> (N,C,H,W)\n",
        "    X_ear = torch.tensor(np.load('X_ear.npy'), dtype=torch.float32).permute(0,3,1,2)\n",
        "    X_tail = torch.tensor(np.load('X_tail.npy'), dtype=torch.float32).permute(0,3,1,2)\n",
        "    X_nose = torch.tensor(np.load('X_nose.npy'), dtype=torch.float32).permute(0,3,1,2)\n",
        "    y = torch.tensor(np.load('labels.npy'), dtype=torch.long)\n",
        "\n",
        "    # 分割訓練與驗證\n",
        "    X_ear_train, X_ear_val, X_tail_train, X_tail_val, X_nose_train, X_nose_val, y_train, y_val = train_test_split(\n",
        "        X_ear, X_tail, X_nose, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    train_dataset = DogDataset(X_ear_train, X_tail_train, X_nose_train, y_train)\n",
        "    val_dataset = DogDataset(X_ear_val, X_tail_val, X_nose_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 設定裝置與模型\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = MultiInputCNN().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    # 訓練迴圈\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "\n",
        "        for (ear, tail, nose), labels in train_loader:\n",
        "            ear, tail, nose, labels = ear.to(device), tail.to(device), nose.to(device), labels.to(device)\n",
        "            outputs = model(ear, tail, nose)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * labels.size(0)\n",
        "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for (ear, tail, nose), labels in val_loader:\n",
        "                ear, tail, nose, labels = ear.to(device), tail.to(device), nose.to(device), labels.to(device)\n",
        "                outputs = model(ear, tail, nose)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * labels.size(0)\n",
        "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "        train_loss /= len(train_dataset)\n",
        "        val_loss /= len(val_dataset)\n",
        "        train_acc = train_correct / len(train_dataset)\n",
        "        val_acc = val_correct / len(val_dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | Val Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # 早停\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # 預測階段\n",
        "    test_df = pd.read_csv(os.path.join(download_dir, 'sample_submission.csv'))\n",
        "\n",
        "    X_test_ear_np, X_test_nose_np, X_test_tail_np = prepare_data_with_local(\n",
        "        os.path.join(download_dir, 'test'), test_df, pytorch_image_size\n",
        "    )\n",
        "\n",
        "    X_test_ear_tensor = torch.tensor(X_test_ear_np, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "    X_test_nose_tensor = torch.tensor(X_test_nose_np, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "    X_test_tail_tensor = torch.tensor(X_test_tail_np, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "\n",
        "    test_dataset = TestDogDataset(X_test_ear_tensor, X_test_tail_tensor, X_test_nose_tensor)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ear, tail, nose in test_loader:\n",
        "            ear, tail, nose = ear.to(device), tail.to(device), nose.to(device)\n",
        "            outputs = model(ear, tail, nose)\n",
        "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds_probs.append(probs)\n",
        "\n",
        "    preds_probs = np.concatenate(all_preds_probs, axis=0)\n",
        "    class_names = list(test_df.columns[1:])  # 除去id欄位\n",
        "\n",
        "    submission = pd.DataFrame(preds_probs, columns=class_names)\n",
        "    submission.insert(0, 'id', test_df['id'])\n",
        "    submission.to_csv('submission_multi_input_cnn.csv', index=False)\n",
        "\n",
        "    print(\"✅ Submission file created: submission_multi_input_cnn.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OW7uFwkPycs",
        "outputId": "153bebd1-2e41-42a0-b817-c995c1ca4bda"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ 你必須自行實作此函數以萃取耳朵、尾巴、鼻子圖像區塊。\n",
            "暫時使用虛擬資料作為示範，目標圖片尺寸：(64, 64)\n",
            "Epoch 1: Train Loss=4.8796, Acc=0.0037 | Val Loss=4.7948, Acc=0.0100\n",
            "Epoch 2: Train Loss=3.0262, Acc=0.3125 | Val Loss=4.8458, Acc=0.0100\n",
            "Epoch 3: Train Loss=1.3592, Acc=0.6700 | Val Loss=4.9387, Acc=0.0050\n",
            "Epoch 4: Train Loss=0.5972, Acc=0.8500 | Val Loss=4.9782, Acc=0.0050\n",
            "Epoch 5: Train Loss=0.2949, Acc=0.9237 | Val Loss=4.8903, Acc=0.0050\n",
            "Epoch 6: Train Loss=0.2218, Acc=0.9437 | Val Loss=4.8434, Acc=0.0100\n",
            "Epoch 7: Train Loss=0.1599, Acc=0.9550 | Val Loss=4.8418, Acc=0.0100\n",
            "Epoch 8: Train Loss=0.1178, Acc=0.9688 | Val Loss=4.8666, Acc=0.0100\n",
            "Epoch 9: Train Loss=0.1109, Acc=0.9637 | Val Loss=4.8948, Acc=0.0050\n",
            "Epoch 10: Train Loss=0.0710, Acc=0.9862 | Val Loss=4.8912, Acc=0.0050\n",
            "Epoch 11: Train Loss=0.0814, Acc=0.9775 | Val Loss=4.8823, Acc=0.0100\n",
            "Early stopping triggered.\n",
            "⚠️ 你必須自行實作此函數以萃取測試集耳朵、尾巴、鼻子圖像區塊。\n",
            "✅ Submission file created: submission_multi_input_cnn.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # 預測階段\n",
        "    test_df = pd.read_csv(os.path.join(download_dir, 'sample_submission.csv'))\n",
        "\n",
        "    X_test_ear_np, X_test_nose_np, X_test_tail_np = prepare_data_with_local(\n",
        "        os.path.join(download_dir, 'test'), test_df, pytorch_image_size\n",
        "    )\n",
        "\n",
        "    X_test_ear_tensor = torch.tensor(X_test_ear_np, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "    X_test_nose_tensor = torch.tensor(X_test_nose_np, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "    X_test_tail_tensor = torch.tensor(X_test_tail_np, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "\n",
        "    test_dataset = TestDogDataset(X_test_ear_tensor, X_test_tail_tensor, X_test_nose_tensor)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ear, tail, nose in test_loader:\n",
        "            ear, tail, nose = ear.to(device), tail.to(device), nose.to(device)\n",
        "            outputs = model(ear, tail, nose)\n",
        "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds_probs.append(probs)\n",
        "\n",
        "    preds_probs = np.concatenate(all_preds_probs, axis=0)\n",
        "    class_names = list(test_df.columns[1:])  # 除去id欄位\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "Hu4STQ-TYSZY",
        "outputId": "406689b9-ec21-4177-8670-67a87e6bb903"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-32-e736a7fa61e5>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-e736a7fa61e5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    test_df = pd.read_csv(os.path.join(download_dir, 'sample_submission.csv'))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 讀取 sample_submission.csv 作為格式參考\n",
        "test_df = pd.read_csv('dog-breed-identification/sample_submission.csv')\n",
        "\n",
        "# --- 假設函數：準備耳朵、尾巴、鼻子的測試圖像 ---\n",
        "def prepare_data_with_local(test_image_dir, test_df, image_size):\n",
        "    \"\"\"\n",
        "    虛擬實作：準備測試圖片的特徵（耳、鼻、尾）區塊。請自行更換為實際的圖像處理邏輯。\n",
        "    \"\"\"\n",
        "    print(\"✅ Preparing test data... (Placeholder version)\")\n",
        "    num_test_samples = len(test_df)\n",
        "    dummy_shape = (num_test_samples, image_size[0], image_size[1], 3)\n",
        "    X_test_ear_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_test_nose_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    X_test_tail_dummy = np.random.rand(*dummy_shape).astype(np.float32)\n",
        "    return X_test_ear_dummy, X_test_nose_dummy, X_test_tail_dummy\n",
        "\n",
        "# 假設圖像大小定義如下（與訓練一致）\n",
        "pytorch_image_size = (64, 64)\n",
        "\n",
        "# 呼叫資料準備函數\n",
        "X_test_ear_np, X_test_nose_np, X_test_tail_np = prepare_data_with_local(\n",
        "    'dog-breed-identification/test', test_df, pytorch_image_size\n",
        ")\n",
        "\n",
        "# 將 numpy 陣列轉為 torch tensor 並調整為 [B, C, H, W] 格式\n",
        "X_test_ear_tensor = torch.tensor(X_test_ear_np, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "X_test_nose_tensor = torch.tensor(X_test_nose_np, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "X_test_tail_tensor = torch.tensor(X_test_tail_np, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "\n",
        "# --- 定義 Dataset ---\n",
        "class TestDogDataset(Dataset):\n",
        "    def __init__(self, ears, tails, noses):\n",
        "        self.ears = ears\n",
        "        self.tails = tails\n",
        "        self.noses = noses\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ears)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.ears[idx], self.tails[idx], self.noses[idx]\n",
        "\n",
        "# 建立 DataLoader\n",
        "test_dataset = TestDogDataset(X_test_ear_tensor, X_test_tail_tensor, X_test_nose_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# --- 預測階段 ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_preds_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for ear, tail, nose in test_loader:\n",
        "        ear, tail, nose = ear.to(device), tail.to(device), nose.to(device)\n",
        "        outputs = model(ear, tail, nose)\n",
        "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "        all_preds_probs.append(probs)\n",
        "\n",
        "# 合併所有 batch 預測\n",
        "preds_probs = np.concatenate(all_preds_probs, axis=0)\n",
        "\n",
        "# ✅ 從 sample_submission 取得正確的品種欄位名稱\n",
        "class_names = list(test_df.columns[1:])  # 忽略 'id'\n",
        "\n",
        "# 構建提交 DataFrame\n",
        "submission = pd.DataFrame(preds_probs, columns=class_names)\n",
        "submission.insert(0, 'id', test_df['id'])  # 插入 id 欄位\n",
        "\n",
        "# 儲存為 CSV 檔案\n",
        "submission.to_csv('submission_multi_input_cnn.csv', index=False)\n",
        "\n",
        "print(\"✅ Submission file created: submission_multi_input_cnn.csv\")\n"
      ],
      "metadata": {
        "id": "4TqcRVqEJblr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcbb3408-10fa-45c5-b14f-c7a54fbe42bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preparing test data... (Placeholder version)\n",
            "✅ Submission file created: submission_multi_input_cnn.csv\n"
          ]
        }
      ]
    }
  ]
}