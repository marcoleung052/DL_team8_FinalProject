{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8bfd9fd30bb4626b9a11343548b813d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_a966fedc32ed4607b8cca37b93a7d5c1",
            "metadata": [
              {
                "name": "kaggle.json",
                "type": "application/json",
                "size": 64,
                "lastModified": 1745215961986
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_20e8f94c480247a0a3e83440ea57ab86"
          }
        },
        "a966fedc32ed4607b8cca37b93a7d5c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20e8f94c480247a0a3e83440ea57ab86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c8bfd9fd30bb4626b9a11343548b813d",
            "a966fedc32ed4607b8cca37b93a7d5c1",
            "20e8f94c480247a0a3e83440ea57ab86"
          ]
        },
        "id": "ClEec7Vx4FAm",
        "outputId": "694b07f2-7a79-4030-b2d6-831a620400d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8bfd9fd30bb4626b9a11343548b813d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# 上傳檔案（會跳出檔案選擇器）\n",
        "from ipywidgets import FileUpload\n",
        "\n",
        "upload = FileUpload()\n",
        "display(upload)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 假設你只上傳了一個檔案\n",
        "# Check if any file is uploaded and get the filename\n",
        "if upload.value:\n",
        "    # Get the first (and likely only) filename from the dictionary keys\n",
        "    filename = list(upload.value.keys())[0]\n",
        "    fileinfo = upload.value[filename] # Access the file info dictionary using the filename as key\n",
        "else:\n",
        "    print(\"No file uploaded.\")\n",
        "    # Handle the case where no file is uploaded, perhaps by exiting or prompting the user.\n",
        "    # For this example, we'll assume a file was uploaded as per the traceback context.\n",
        "    raise FileNotFoundError(\"No file was uploaded.\")\n",
        "\n",
        "\n",
        "# 顯示內容結構（除錯用）\n",
        "print(fileinfo)\n",
        "\n",
        "# 儲存 kaggle.json\n",
        "# filename is already obtained above\n",
        "content = fileinfo['content']\n",
        "\n",
        "kaggle_dir = Path.home() / \".kaggle\"\n",
        "kaggle_dir.mkdir(exist_ok=True)\n",
        "\n",
        "kaggle_json_path = kaggle_dir / \"kaggle.json\"\n",
        "with open(kaggle_json_path, \"wb\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "# 設定權限（Linux/macOS 建議）\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "print(f\"{filename} 已成功儲存至 {kaggle_json_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwYKgADr4I-N",
        "outputId": "5505f68c-9d9e-4ef6-fd6e-0a672d3945fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'metadata': {'name': 'kaggle.json', 'type': 'application/json', 'size': 64, 'lastModified': 1745215961986}, 'content': b'{\"username\":\"suchiwen\",\"key\":\"01a925cee9e9e9d232008524b0434fb9\"}'}\n",
            "kaggle.json 已成功儲存至 /root/.kaggle/kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list -s cifar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7YR10jR4SsF",
        "outputId": "a1a206ed-16ec-4d0f-9977-b054648a667c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                     title                                                  size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------  ----------------------------------------------  -----------  --------------------------  -------------  ---------  ---------------  \n",
            "fedesoriano/cifar100                                    CIFAR-100 Python                                  168517809  2020-12-26 08:37:10.143000          12303        176  1.0              \n",
            "pankrzysiu/cifar10-python                               CIFAR-10 Python                                   340613496  2018-01-27 13:42:40.967000          14829        252  0.75             \n",
            "petitbonney/cifar10-image-recognition                   CIFAR-10                                         1007971063  2019-10-01 12:50:23.227000           2890         27  0.8235294        \n",
            "valentynsichkar/cifar10-preprocessed                    CIFAR10 Preprocessed                             1227571899  2019-07-13 13:43:11.030000           3369         65  1.0              \n",
            "swaroopkml/cifar10-pngs-in-folders                      CIFAR-10 PNGs in folders                          293479104  2019-02-10 11:16:19.507000          13510         96  0.6875           \n",
            "fedesoriano/cifar10-python-in-csv                       CIFAR-10 Python in CSV                            218807675  2021-06-22 09:07:55.670000           7059         53  1.0              \n",
            "ayush1220/cifar10                                       CIFAR-10                                          146259525  2022-01-11 21:03:48.427000           1297         12  0.6875           \n",
            "alincijov/cifar-100                                     CIFAR-100                                         168517809  2020-09-13 13:40:25.763000            370         16  0.625            \n",
            "quanbk/cifar10                                          Cifar-10                                          170062484  2017-11-22 01:12:26.240000           3633         26  0.25             \n",
            "emadtolba/cifar10-comp                                  Cifar-10 comp                                     706709151  2018-09-09 02:55:22.263000            384         12  0.64705884       \n",
            "birdy654/cifake-real-and-ai-generated-synthetic-images  CIFAKE: Real and AI-Generated Synthetic Images    109625224  2023-03-28 16:00:29.500000          19862        150  0.875            \n",
            "amishfaldu/cifar10dataset                               Cifar-10 Dataset                                 4989376312  2020-10-07 04:30:40.600000            350          6  1.0              \n",
            "ibraheemmoosa/cifar100-256x256                          CIFAR100 256x256                                 2817065558  2021-04-24 17:09:57.140000           1037         23  0.875            \n",
            "jessicali9530/stl10                                     STL-10 Image Recognition Dataset                 2017846807  2018-06-11 03:02:24.153000           6851        125  0.75             \n",
            "oxcdcd/cifar10                                          cifar10                                           183819211  2018-08-12 09:31:35.253000           3406         35  0.1875           \n",
            "gazu468/cifar10-classification-image                    Cifar10 Classification Image                      146259525  2022-10-29 18:47:18.453000            782         14  0.5              \n",
            "pavansanagapati/cifar100                                CIFAR-100                                         337036008  2018-06-21 12:23:51.707000            416          4  0.3125           \n",
            "ekaakurniawan/the-cifar10-dataset                       The CIFAR-10 Dataset                              170063312  2021-04-06 16:57:56.220000            513          4  0.9375           \n",
            "pypiahmad/cifar-100                                     CIFAR-100                                         168517947  2023-10-29 09:53:59.520000             80         11  0.6875           \n",
            "imsparsh/musicnet-dataset                               MusicNet Dataset                                23086004822  2021-02-18 14:12:19.753000          16109        527  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U kaggle\n",
        "!pip install --upgrade pandas\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# 建立 Kaggle 資料夾\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# 下載 Dog Breed Identification 資料集\n",
        "!kaggle competitions download -c dog-breed-identification --force\n",
        "!unzip -oq dog-breed-identification.zip -d dog-breed-identification\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "labels = pd.read_csv('dog-breed-identification/labels.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kMGSqp_4W7t",
        "outputId": "3bb969f0-0873-4d95-cb92-051f8ad8316f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "Downloading dog-breed-identification.zip to /content\n",
            " 98% 675M/691M [00:03<00:00, 115MB/s] \n",
            "100% 691M/691M [00:03<00:00, 196MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 訓練設定 ==========\n",
        "import torch\n",
        "from torch.utils.data import random_split, DataLoader, WeightedRandomSampler\n",
        "from collections import Counter\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd # Ensure pandas is imported if not already\n",
        "import numpy as np # Ensure numpy is imported if not already\n",
        "import shutil # Ensure shutil is imported if not already\n",
        "\n",
        "# Assuming DogBreedImageFolder class and DogCNN model are defined in previous cells and working correctly.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transform\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2),\n",
        "    transforms.RandomAffine(15, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Dataset & Split\n",
        "# Ensure the 'dog_images/train' directory is correctly populated by previous steps\n",
        "dataset = DogBreedImageFolder(\"dog_images/train\", transform=train_transform)\n",
        "val_ratio = 0.2\n",
        "val_size = int(len(dataset) * val_ratio)\n",
        "train_size = len(dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "val_dataset.dataset.transform = val_transform  # 替換驗證用 transform\n",
        "\n",
        "# Weighted Sampler\n",
        "train_indices = train_dataset.indices\n",
        "# Access the labels from the original dataset using the training indices\n",
        "train_labels = [dataset.labels[i] for i in train_indices]\n",
        "label_counts = Counter(train_labels)\n",
        "\n",
        "# Calculate sample weights for the WeightedRandomSampler\n",
        "sample_weights = [1.0 / label_counts[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "# Calculate class weights for the loss function (if needed, e.g., for CrossEntropyLoss or FocalLoss)\n",
        "# This is based on the counts in the training subset\n",
        "num_classes = len(dataset.class_names) # Get the number of classes from the dataset\n",
        "class_sample_counts = [label_counts.get(i, 0) for i in range(num_classes)] # Use 0 for classes not present in train split initially\n",
        "# Avoid division by zero for classes not in the training split (though rare in this case)\n",
        "weights = torch.tensor([1.0 / (c if c > 0 else 1.0) for c in class_sample_counts], dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Print label_counts and weights for verification\n",
        "print(\"Label counts in training set:\", label_counts)\n",
        "print(\"Class weights tensor for loss:\", weights)\n",
        "\n",
        "# Note: The earlier code block that caused the error and recalculated label_counts and class_weights\n",
        "# should be removed or commented out as it was redundant and incorrect in its placement.\n",
        "# The calculation of weights for the FocalLoss is correctly placed later in the notebook."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtBNa3ZM5gFB",
        "outputId": "45ac0124-c6a3-4271-97dc-a25e7892db7b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label counts in training set: Counter({97: 106, 73: 98, 87: 95, 42: 95, 11: 93, 52: 92, 100: 90, 1: 88, 7: 88, 69: 87, 109: 85, 6: 85, 3: 83, 68: 83, 26: 83, 61: 82, 80: 82, 13: 81, 59: 80, 94: 79, 9: 78, 93: 77, 75: 76, 84: 76, 101: 76, 70: 75, 30: 75, 98: 75, 55: 74, 114: 74, 86: 74, 117: 74, 88: 73, 35: 73, 17: 73, 8: 72, 14: 72, 78: 72, 28: 71, 40: 70, 90: 70, 95: 70, 31: 70, 33: 70, 118: 70, 119: 69, 2: 69, 102: 69, 82: 69, 60: 69, 79: 68, 71: 67, 56: 67, 20: 67, 15: 66, 92: 66, 39: 66, 96: 65, 53: 65, 62: 65, 19: 65, 37: 65, 89: 65, 67: 65, 0: 65, 57: 64, 27: 64, 32: 64, 10: 64, 107: 64, 105: 64, 25: 64, 111: 64, 36: 63, 54: 63, 110: 63, 116: 63, 104: 63, 58: 63, 91: 63, 81: 62, 21: 62, 99: 61, 44: 61, 76: 61, 38: 61, 50: 61, 64: 61, 74: 61, 12: 61, 29: 61, 115: 61, 18: 60, 85: 60, 34: 60, 4: 59, 108: 59, 77: 59, 63: 59, 45: 58, 51: 58, 112: 58, 47: 58, 5: 57, 24: 57, 66: 57, 65: 56, 103: 56, 113: 56, 48: 55, 83: 55, 46: 55, 41: 54, 49: 54, 106: 54, 23: 54, 43: 52, 72: 52, 16: 52, 22: 50})\n",
            "Class weights tensor for loss: tensor([0.0154, 0.0114, 0.0145, 0.0120, 0.0169, 0.0175, 0.0118, 0.0114, 0.0139,\n",
            "        0.0128, 0.0156, 0.0108, 0.0164, 0.0123, 0.0139, 0.0152, 0.0192, 0.0137,\n",
            "        0.0167, 0.0154, 0.0149, 0.0161, 0.0200, 0.0185, 0.0175, 0.0156, 0.0120,\n",
            "        0.0156, 0.0141, 0.0164, 0.0133, 0.0143, 0.0156, 0.0143, 0.0167, 0.0137,\n",
            "        0.0159, 0.0154, 0.0164, 0.0152, 0.0143, 0.0185, 0.0105, 0.0192, 0.0164,\n",
            "        0.0172, 0.0182, 0.0172, 0.0182, 0.0185, 0.0164, 0.0172, 0.0109, 0.0154,\n",
            "        0.0159, 0.0135, 0.0149, 0.0156, 0.0159, 0.0125, 0.0145, 0.0122, 0.0154,\n",
            "        0.0169, 0.0164, 0.0179, 0.0175, 0.0154, 0.0120, 0.0115, 0.0133, 0.0149,\n",
            "        0.0192, 0.0102, 0.0164, 0.0132, 0.0164, 0.0169, 0.0139, 0.0147, 0.0122,\n",
            "        0.0161, 0.0145, 0.0182, 0.0132, 0.0167, 0.0135, 0.0105, 0.0137, 0.0154,\n",
            "        0.0143, 0.0159, 0.0152, 0.0130, 0.0127, 0.0143, 0.0154, 0.0094, 0.0133,\n",
            "        0.0164, 0.0111, 0.0132, 0.0145, 0.0179, 0.0159, 0.0156, 0.0185, 0.0156,\n",
            "        0.0169, 0.0118, 0.0159, 0.0156, 0.0172, 0.0179, 0.0135, 0.0164, 0.0159,\n",
            "        0.0135, 0.0143, 0.0145], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset 建立\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# Assuming the 'labels' dataframe is already loaded from labels.csv in a previous cell\n",
        "# Example (if labels was not loaded previously, add this):\n",
        "# labels = pd.read_csv('dog-breed-identification/labels.csv')\n",
        "\n",
        "\n",
        "class DogBreedDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        self.dataframe = dataframe.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        # Assuming 'label' column in dataframe contains string labels\n",
        "        # We need to map these to integers for training\n",
        "        unique_labels = sorted(self.dataframe['label'].unique())\n",
        "        self.label_to_int = {label: i for i, label in enumerate(unique_labels)}\n",
        "        self.int_to_label = {i: label for label, i in self.label_to_int.items()}\n",
        "        self.dataframe['label_int'] = self.dataframe['label'].map(self.label_to_int)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image_id = row['id']\n",
        "        # Use the integer label for training\n",
        "        label_int = row['label_int']\n",
        "        # Assuming images are in 'image_dir' and named as 'id.jpg'\n",
        "        img_path = os.path.join(self.image_dir, f'{image_id}.jpg')\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label_int"
      ],
      "metadata": {
        "id": "iSlkQfuTDExT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Step 2: 自定義 CNN 模型 ============\n",
        "# ============ Step 2: 自定義 CNN 模型 ============\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        # Calculate the size of the flattened features\n",
        "        # Input size is 224x224, after 3 MaxPool(2) layers, spatial size becomes 224 / (2*2*2) = 224 / 8 = 28\n",
        "        # The number of channels is 128\n",
        "        flattened_size = 128 * 28 * 28\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            # Update the input features to the first Linear layer\n",
        "            nn.Linear(flattened_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Re-instantiate the model after modifying the class definition\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Ensure num_classes is correctly defined from the dataset in a previous cell\n",
        "# If not defined, you might need to define it here, e.g., num_classes = 120\n",
        "model = SimpleCNN(num_classes).to(device)"
      ],
      "metadata": {
        "id": "WlGyLtCs4iRP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 8. 訓練主程式 ==========\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss(weight=weights_tensor) # Original line causing error\n",
        "criterion = nn.CrossEntropyLoss(weight=weights) # Use the 'weights' tensor calculated earlier\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            outputs = model(x)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {total_loss:.4f} | Val Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk5RkRyQ712G",
        "outputId": "39dbade0-ada2-4257-ed32-ef55ea0d63cd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:36<00:00, 14.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Loss: 2642.5781 | Val Acc: 0.0098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:36<00:00, 14.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 | Train Loss: 2449.3411 | Val Acc: 0.0059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:36<00:00, 13.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 | Train Loss: 2448.2587 | Val Acc: 0.0059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:36<00:00, 14.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 | Train Loss: 2448.8330 | Val Acc: 0.0059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:36<00:00, 14.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 | Train Loss: 2447.0009 | Val Acc: 0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:36<00:00, 13.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 | Train Loss: 2447.6960 | Val Acc: 0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:35<00:00, 14.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 | Train Loss: 2447.3743 | Val Acc: 0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:36<00:00, 14.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 | Train Loss: 2446.9847 | Val Acc: 0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:35<00:00, 14.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 | Train Loss: 2446.3154 | Val Acc: 0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 512/512 [00:36<00:00, 13.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 | Train Loss: 2446.6181 | Val Acc: 0.0088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 預測\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_ids in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1).cpu().numpy()  # 機率\n",
        "\n",
        "        for img_id, prob in zip(img_ids, probs):\n",
        "            all_predictions.append([img_id] + prob.tolist())\n",
        "\n",
        "# class_names 來自 dataset.class_names\n",
        "class_names = dataset.class_names\n",
        "\n",
        "# 寫入 submission.csv\n",
        "submission_df = pd.DataFrame(all_predictions, columns=['id'] + class_names)\n",
        "submission_df['id'] = submission_df['id'].str.replace('.jpg', '', regex=False)  # 去掉副檔名\n",
        "submission_df.to_csv(\"submission_cnn.csv\", index=False)\n",
        "\n",
        "print(\"submission_cnn.csv 已產生完成，可上傳至 Kaggle 評分。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAMqldmaFfv_",
        "outputId": "77edd62a-9bb4-4adb-a1f8-6189389fb0b5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 324/324 [00:48<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "submission.csv 已產生完成，可上傳至 Kaggle 評分。\n"
          ]
        }
      ]
    }
  ]
}